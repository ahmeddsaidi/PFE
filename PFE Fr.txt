\documentclass[12pt,a4paper]{report}

% Packages essentiels
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{float}
\usepackage[skip=10pt]{parskip}
\usepackage{pdflscape}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, trees, shapes, positioning}


% Configuration de la page
\geometry{a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\definecolor{lightgray}{rgb}{0.97,0.97,0.97}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{myblue}{rgb}{0,0,0.8}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mypurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{lightgray},
    commentstyle=\color{mygreen}\itshape,
    keywordstyle=\color{myblue}\bfseries,
    stringstyle=\color{mypurple},
    numberstyle=\tiny\color{mygray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    language=Python,
    frame=single,
    numbers=left,
    numbersep=8pt,
    showstringspaces=false,
    tabsize=4
}

% Configuration des en-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{\textit{Rapport de Projet de Fin d'Études - \the\year}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Style des titres
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\color{blue}}
{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titlespacing*{\chapter}{0pt}{50pt}{40pt}

% Informations du document
\title{
    \vspace{2cm}
    \LARGE{\textbf{RAPPORT DE PROJET DE FIN D'ÉTUDES}}\\
    \vspace{1cm}
    \large{Automatisation et Amélioration du Processus de Gestion des Ressources utilisant les Outils du Traitement de Données et l'IA}\\
    \vspace{2cm}
}

\author{
    \begin{tabular}{c}
        SAIDI Ahmed\\
        \textit{Ingénierie Data Science and Cloud Computing}
    \end{tabular}
}

\date{
    \vspace{1cm}
    Année Académique 2024--2025\\
    \vspace{2cm}
}

\begin{document}

% Page de garde
\begin{titlepage}
    \begin{center}
        \begin{figure}[h]
            \centering
            \begin{minipage}{0.27\textwidth}
                \includegraphics[width=\textwidth]{ensa (1).png}
            \end{minipage}
            \hfill
            \begin{minipage}{0.20\textwidth}
                \includegraphics[width=\textwidth]{ump.png}
            \end{minipage}
        \end{figure}
        
        \textsc{\Large Ecole Nationale des Sciences Appliquées d'Oujda}\\[0.5cm]
        \textsc{\large Ingénierie Data Science and Cloud Computing}\\[1cm]
        
        \hrule
        \vspace{0.5cm}
        {\huge\bfseries RAPPORT DE PROJET DE FIN D'ÉTUDES}\\[0.5cm]
        \hrule
        \vspace{1cm}

        {\setlength{\baselineskip}{1.5em}
        {\LARGE\textbf{
        L'IA \& BI dans l'Automatisation et l'Amélioration des Processus \\
        de Gestion et Surveillance \\
        }}} \\[1cm]
        
        {\large Réalisé par:}\\[0.3cm]
        {\Large\textbf{SAIDI Ahmed}}\\[0.8cm]
        
        {\large Pour l'obtention du:}\\[0.3cm]
        {\Large\textbf{Diplôme d'Ingénieur d'État}}\\[0.8cm]
        
        \vfill
        
        \begin{tabular}{rl}
            \textbf{Encadrant académique:} & M. Abdelmounaim KERKRI\\
            \textbf{Encadrant professionnel:} & Mme. Hiba MADRANE\\
            \textbf{Organisme d'accueil:} & JESA
        \end{tabular}
        
        \vfill
        
        {\large Année Académique 2024--2025}
    \end{center}
\end{titlepage}

% Remerciements
\chapter*{Remerciements}
\addcontentsline{toc}{chapter}{Remerciements}

Je tiens à exprimer ma profonde gratitude à tous ceux qui ont contribué à la réalisation de ce projet de fin d’études, une étape clé de mon parcours académique.

Mes sincères remerciements vont à \textbf{Mme Hiba MADRANE}, ma tutrice professionnelle à JESA, pour son accueil chaleureux, ses conseils stratégiques et son soutien constant, dont les conseils ont permis de concrétiser l’agent IA, l’application web et les tableaux de bord Power BI. Je remercie également \textbf{M. Abdelmounaim KERKRI}, mon encadrant académique, pour sa guidance rigoureuse, ses remarques pertinentes et sa disponibilité tout au long du projet.

Je suis reconnaissant envers l’équipe du département Field Services (FS) de JESA, pour leur collaboration et leurs retours précieux qui ont enrichi ce travail. Un merci particulier à mes collègues stagiaires, \textbf{Nassrou-Eddine} et \textbf{Fadoua}, pour leur esprit d’équipe et leur soutien amical.

Je rends hommage au corps professoral de l’\textbf{École Nationale des Sciences Appliquées d’Oujda}, notamment à la filière Ingénierie Data Science and Cloud Computing, pour la qualité de la formation dispensée, qui m’a doté des compétences nécessaires à la réalisation de ce projet.

Enfin, je dédie ce travail à ma famille et à mes amis proches, \textbf{Mouaad, Zakaria, Ahmed, Mohammed, Anass, Soufian}, pour leur soutien indéfectible et leurs encouragements constants. Ce projet est le fruit d’un effort collectif, et je suis honoré de leur appui.

% Dédicace
\chapter*{Dédicace}
\addcontentsline{toc}{chapter}{Dédicace}

\itshape
À la mémoire de ma chère mère, dont l’amour et les prières continuent de m’accompagner chaque jour.\\[0.5em]
À mon père bien-aimé, pour son courage, ses sacrifices et sa confiance inébranlable.\\[0.5em]
À mon frère et à mes proches, pour leur présence discrète mais essentielle, leur soutien fidèle et leurs encouragements inestimables.\\[0.5em]

Une dédicace particulière à \textbf{M. Abdelmounaim KERKRI} pour sa bienveillance et son accompagnement tout au long de ce projet,\\
et à \textbf{Mme Hiba MADRANE}, pour sa rigueur professionnelle, ses conseils avisés et sa disponibilité.\\[0.5em]

À mes amis fidèles et présents dans toutes les étapes de ma vie : \textbf{Mouaad, Anass, Mohammed, Ahmed, Zakaria, Soufian},\\
merci pour votre amitié sincère, vos éclats de rire, et votre soutien indéfectible.\\[0.5em]

À mes camarades de classe, à ceux de l’école, du lycée et de l’université,\\
chaque moment partagé a été une pierre dans la construction de mon parcours.\\[0.5em]

À ma grande famille, pour ses encouragements et son amour inconditionnel.\\[0.5em]

Enfin, à toutes les personnes qui ont cru en moi, même dans mes doutes,\\
je vous dédie ce travail avec reconnaissance, respect et beaucoup d’affection.


\upshape
% Résumé
\chapter*{Résumé}
\addcontentsline{toc}{chapter}{Résumé}

Ce rapport présente le travail réalisé dans le cadre d’un stage de fin d’études portant sur la conception et le développement d’un système intelligent centré autour d’un agent d’IA conversationnel capable d’extraire des données à partir d’une base SQL, de générer des rapports et de les envoyer automatiquement par e-mail, le tout à partir de requêtes en langage naturel. L’agent repose sur le modèle LLaMA 3 (70B) et est hébergé via l’API Groq, choisie pour sa performance et son coût en l’absence d’infrastructure GPU locale.

Une application web a été développée autour de cet agent, offrant une interface utilisateur semblable à ChatGPT, ainsi qu’un tableau de bord de KPI, une page de gestion des rapports générés, une visualisation embarquée Power BI, et un espace d’administration avec supervision système et gestion des utilisateurs. Deux projets de dashboards ont également été réalisés, dont un construit entièrement depuis un fichier Excel (dashboard vert) et un autre amélioré à partir d’un existant (dashboard brun).

Enfin, une solution d’automatisation des tâches répétitives de type copier-coller entre fichiers Excel a été développée, intégrant un moteur de correspondance flexible basé sur des règles définissables par l’utilisateur. L’évaluation du système a montré des performances satisfaisantes en termes de précision, de réactivité et de gain de productivité. Le rapport inclut une étude théorique approfondie, une analyse architecturale complète et des perspectives d’évolution concrètes.

\textbf{Mots-clés} : IA conversationnelle, NL2SQL, LLaMa3, Groq API, Tableau de bord, Power BI, Automatisation Excel, Mappage de données, Prompt Engineering, Business Intelligence, Application Web.

% Abstract
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This report details the final-year internship project focused on designing and developing an intelligent system built around a conversational AI agent capable of extracting data from an SQL database, generating reports, and sending them automatically by email, all based on natural language user input. The agent is powered by LLaMA 3 (70B) and hosted through the Groq API, selected for its high speed and cost-efficiency given the absence of local GPU resources.

A web application was developed around this agent, offering a ChatGPT-like user interface, a KPI dashboard page, a generated report management page, an embedded Power BI viewer, and an admin-only section for system monitoring and user management. Additionally, two dashboard projects were conducted: one built entirely from Excel source data (green dashboard), and another redesigned from an existing version (brown dashboard).

Finally, an Excel automation engine was built to eliminate repetitive copy-paste tasks between files, featuring a configurable mapping engine that handles column renaming and value harmonization. The system's evaluation showed promising results in terms of accuracy, latency, and productivity gains. The report includes a thorough theoretical review, complete system architecture, and concrete recommendations for future enhancements.

\textbf{Keywords} : Conversational AI, NL2SQL, LLaMa3, Groq API, Dashboard, Power BI, Excel Automation, Data Mapping, Prompt Engineering, Business Intelligence, Web Application.

% Liste des abréviations
\chapter*{Liste des abréviations et acronymes}
\addcontentsline{toc}{chapter}{Liste des abréviations et acronymes}

\begin{tabular}{ll}
    \textbf{Abréviation} & \textbf{Signification} \\
    \hline
    API & Application Programming Interface \\
    BI & Business Intelligence \\
    C\&C & Commissioning and Completion \\
    CSV & Comma-Separated Values \\
    DAX & Data Analysis Expressions \\
    FAISS & Facebook AI Similarity Search \\
    FS & Field Services \\
    Groq & General-purpose compute platform for AI inference \\
    IA & Intelligence Artificielle \\
    JS & JavaScript \\
    LLM & Large Language Model \\
    LLaMA & Large Language Model Meta AI \\
    LoRA & Low-Rank Adaptation \\
    MLP & Multi-Layer Perceptron \\
    MIT & Massachusetts Institute of Technology \\
    NLP & Natural Language Processing \\
    NL2SQL & Natural Language to Structured Query Language \\
    PDF & Portable Document Format \\
    PFE & Projet de Fin d’Études \\
    QLoRA & Quantized Low-Rank Adaptation \\
    RAG & Retrieval-Augmented Generation \\
    SC & Smart Commissioning \\
    SQL & Structured Query Language \\
    UI & User Interface \\
    UX & User Experience \\
    UUID & Universally Unique Identifier \\
\end{tabular}

\newpage
% Table des matières
\tableofcontents
\clearpage

% Liste des figures
\listoffigures
\clearpage

% Liste des tableaux
\listoftables
\clearpage

% Chapitre 1: Introduction générale
\chapter{Introduction}

\section{Contexte du stage}

L’intelligence artificielle, et plus précisément les agents conversationnels intelligents, occupent aujourd’hui une place centrale dans les stratégies numériques des entreprises. Grâce aux avancées récentes dans le domaine du traitement du langage naturel (NLP) et à l'émergence des grands modèles de langage (LLM) comme GPT, LLaMA, Claude ou Mistral, il est désormais possible de concevoir des systèmes capables de comprendre et d’interagir avec les utilisateurs de manière fluide et pertinente. Dans ce contexte technologique en pleine effervescence, j’ai intégré une équipe avec une mentalité d'innovation du département \textbf{Field Services}, dans le cadre de la digitaliation ou bien la transformation digital interne, de l’entreprise \textbf{JESA}, afin de contribuer au développement de solutions innovantes basées sur l’IA générative et autre technologies du traitement de données  pour l'automatisation.

Ce stage s’est inscrit dans une volonté de transformer certains processus internes, jugés chronophages ou sujets à erreurs, en workflows automatisés, tout en mettant à disposition des utilisateurs métiers des interfaces intuitives et intelligentes.


\section{Architecture de l'Organisme d'Accueil}

\subsection{JESA - The Solution Company For Africa}

JESA (Jacobs Engineering SA) est une entreprise marocaine de référence dans le domaine de l’ingénierie, de la gestion de projets et du conseil stratégique. Fondée en 2010 à travers une joint-venture entre OCP Group et Jacobs Engineering Group, appartenant à Worley pour le moment, JESA s’est imposée comme un acteur majeur au Maroc et sur l’ensemble du continent africain.

\begin{figure}[H]
\centering
\includegraphics[width=0.4 \textwidth]{JESA (1) (1).png}
\caption{JESA logo}
\label{fig:jesa}
\end{figure}

L’entreprise propose des solutions intégrées couvrant tout le cycle de vie d’un projet — de la conception à la réalisation, en passant par l’optimisation des opérations. Son expertise englobe plusieurs secteurs stratégiques, notamment l’industrie chimique, les infrastructures, l’énergie, le transport, l’eau et l’environnement.

Avec un effectif de plusieurs milliers de collaborateurs et des implantations dans plusieurs pays africains, JESA met en avant une approche axée sur l’innovation, la durabilité et le développement local. L’ambition de l’entreprise est claire : être « The Solution Company For Africa », c’est-à-dire une entité capable d’accompagner la transformation du continent grâce à des projets structurants et responsables.


\subsection{Département du Field Service}

Le département du Field Service est responsable de l'exécution des projets directement sur les sites des clients. Il joue un rôle crucial dans la pénultième phase des projets, en assurant la mise en œuvre, la supervision, et le bon déroulement des opérations sur le terrain.


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{hirarchy.jpg}
\caption{Hiérarchie de la département FS}
\label{fig:hiérarchie_fs}
\end{figure}


\subsubsection{Recrutement (Staffing)}

La section Recrutement, aussi appelée Staffing, est chargée de sélectionner, mobiliser, embaucher et affecter les ressources humaines nécessaires aux différents projets du département. Elle veille à ce que les équipes sur le terrain soient qualifiées, disponibles et conformes aux exigences spécifiques de chaque mission.

\subsubsection{Completion \& Commissioning (C\&C)}

La division Completion \& Commissioning (C\&C) intervient dans la dernière phase des projets. Elle est responsable de la finalisation des travaux, des vérifications de qualité, des tests fonctionnels et de la mise en service des systèmes. Son objectif est d'assurer que tous les équipements et installations répondent aux critères de performance définis avant leur livraison au client.


\section{Problématique}

Dans la majorité des environnements professionnels, et notamment au sein d’organisations complexes comme JESA, l’accès aux données stratégiques soulève plusieurs difficultés récurrentes. Les directions et services métiers sollicitent fréquemment des extractions spécifiques de données, destinées à alimenter des rapports, à prendre des décisions ou à satisfaire des demandes clients ou hiérarchiques. Or, ces demandes suivent souvent un schéma désorganisé et inefficace.

Typiquement, une demande de type « donne-moi les données » engendre une chaîne de requêtes manuelles en cascade, adressées à différents intervenants, chacun devant comprendre, reformuler ou retravailler la demande initiale. Cette chaîne peut parfois se perdre en chemin, surtout en l’absence d’un référentiel centralisé (Data Master), où la source des données ou leur signification exacte peut être absente, mal définie ou sujette à interprétation.

Les conséquences sont multiples :
\begin{itemize}
\item Latence importante entre la demande initiale et la livraison du rapport final ;
\item Reproductions répétitives des mêmes tâches pour des requêtes similaires ;
\item Productivité fortement diminuée pour les équipes chargées de ces tâches transverses ;
\item Risques accrus d’erreurs humaines dans la manipulation manuelle des données ;
\item Perte de temps sur des activités à faible valeur ajoutée.
\end{itemize}

Face à cette situation, il est devenu impératif de repenser le processus de communication avec les données. L’objectif est de permettre à chaque utilisateur, selon ses droits, d’accéder directement aux informations souhaitées, en langage naturel, et d’obtenir des réponses instantanées, automatisées et contextualisées.

La problématique peut donc se reformuler de la manière suivante :

\begin{quote}
\textit{Comment concevoir un agent intelligent, fonctionnant 24/7, capable d’interpréter une requête métier en langage naturel, d’interagir avec une base de données dynamique, de générer des rapports pertinents, puis de les restituer de manière conviviale, tout en s'intégrant au rythme de travail et aux exigences d'agilité de l’organisation ?}
\end{quote}

Ce changement de paradigme vise à transformer un processus fragmenté et rigide en un système fluide, interactif et automatisé. En somme, passer du modèle "Before" — où l’accès à la donnée est manuel, lent, répétitif et peu traçable — à un modèle "After" où :
\begin{itemize}
\item L’accès à la donnée est flexible, rapide et contrôlé ;
\item Les réponses sont générées en quelques secondes ;
\item Le processus est entièrement automatisé et traçable ;
\item La communication entre métier et autre est rendue plus naturelle, contextuelle et productive.
\end{itemize}

\section{Objectifs du projet}

L’objectif général de ce projet de fin d’études est de répondre à la problématique identifiée par la mise en œuvre d’une solution intelligente, modulaire, automatisée et adaptée au contexte professionnel de JESA.

Plus précisément, le projet vise à automatiser les échanges entre les "Top Managers" et les "Data Masters" qui sont souvent les opérationnelles dessous, tout en garantissant un accès sécurisé, une qualité de service continue et une ergonomie moderne.

Les objectifs détaillés peuvent être résumés comme suit :

\begin{itemize}
\item \textbf{Intégration de l’IA conversationnelle :} Étudier et implémenter un agent conversationnel basé sur les modèles LLM, capable de comprendre des requêtes en langage naturel et de les transformer automatiquement en requêtes SQL fiables (NL2SQL).

\item \textbf{Automatisation des extractions et des envois :} Permettre à l’agent de générer des rapports, d’envoyer les résultats par e-mail et de répondre de façon autonome 24/7, en s’adaptant aux rythmes spécifiques de travail des équipes.

\item \textbf{Développement d’une application Web moderne :} Concevoir une interface simple en UX, intégrant un historique de requêtes, une page de visualisation de KPIs, une gestion centralisée des rapports générés, une page d’administration (suivi du système, gestion des utilisateurs), et une intégration directe de rapports Power BI.

\item \textbf{Mise en place de deux dashboards métier :} Construire un tableau de bord complet à partir de fichiers Excel pour le monitoring des systèmes C\&C (« Green Dashboard »), et améliorer un tableau de bord existant pour le suivi du Staffing (« Brown Dashboard »).

\item \textbf{Développement d’un Canal de données Excel Automatisé :} Proposer une solution pour automatiser les transferts de données entre fichiers Excel hétérogènes, en gérant les différences de noms de colonnes et de formats de valeurs, réduisant ainsi les tâches manuelles répétitives à forte perte de temps.
\end{itemize}

L’ensemble de ces objectifs converge vers une ambition centrale : automatiser et améliorer la productivité, la réactivité et la fiabilité des processus liés à la donnée pour le département FS dans un environnement dynamique, sans surcharger les équipes métiers avec des outils techniques complexes.

\section{Portée et limites}

Le projet réalisé s'inscrit dans une démarche pragmatique, tenant compte à la fois des attentes fonctionnelles de l’entreprise et des contraintes techniques du terrain. Il couvre plusieurs volets : développement backend, intégration d’API, design d’interfaces, visualisation de données, automatisation de workflows, etc.

\subsection{Portée}

Le périmètre couvert par le stage comprend :

\begin{itemize}
    \item L'utilisation de modèles de langage performants comme LLaMA3 via un API ou localement au but de faire du fine-tuning.
    \item L’intégration complète d’une application web, incluant à la fois une pages d'accueil, une zone de chat, des zones liées aux résultats du chatbot, ainsi qu’un espace administrateur restreint.
    \item L’analyse de fichiers Excel hétérogènes, leur transformation en un schéma en étoile adapté à Power BI, et le développement de visualisations interactives pour le suivi des systèmes (C\&C).
    \item La création d’une interface intuitive d’automatisation des transferts Excel-Excel, avec options de correspondance personnalisée des colonnes et des valeurs.
\end{itemize}

\subsection{Limites}

Certaines limites techniques et organisationnelles ont toutefois été identifiées :

\begin{itemize}
    \item L’absence de ressources matérielles avancées (fort CPU ou GPU) a rendu très difficile à impossible le fine-tuning local de modèles de langage.
    \item Le recours à un API, bien que très rapide, introduit une dépendance à un fournisseur externe, avec des contraintes de coût, de sécurité et de disponibilité.
    \item Les données Excel utilisées présentent souvent des structures incohérentes ou non normalisées, nécessitant des traitements spécifiques au cas par cas.
    \item L'utilisation du cloud public avec AWS ou Azure est difficile vue l'importance de la confidentialité des donnés.
\end{itemize}

\section{Méthodologie adoptée}

Pour mener à bien le projet, une approche incrémentale et itérative a été privilégiée, favorisant des livraisons régulières et des ajustements progressifs en fonction des retours utilisateurs. Cette démarche s’inspire fortement des méthodes agiles, notamment Scrum.

Les étapes principales de la méthodologie sont les suivantes :

\begin{enumerate}
    \item \textbf{Recueil des besoins} : identification des attentes métiers, des cas d’usage, et des données disponibles.
    \item \textbf{Étude de faisabilité} : analyse comparative de solutions techniques possibles (Externe vs Local, API vs fine-tuning).
    \item \textbf{Conception de l’architecture} : modélisation des composants, flux de données, interfaces, et mécanismes d’intégration.
    \item \textbf{Développement} : mise en œuvre des différentes briques (agent IA, app web, dashboards, interface auto-Excel).
    \item \textbf{Tests et validation} : vérification fonctionnelle, validation métier, ajustements en conditions réelles.
    \item \textbf{Documentation} : rédaction de guides utilisateurs, documentation technique, et rapport de stage.
\end{enumerate}

\section{Structure du rapport}

Ce rapport est organisé de manière à guider progressivement le lecteur depuis les fondements théoriques jusqu’aux implémentations concrètes, en passant par les choix technologiques, les défis rencontrés, et les résultats obtenus. Il se structure comme suit :

\begin{itemize}
    \item \textbf{Chapitre 1 : Introduction} – Présentation du contexte, de JESA et ses départements, de la problématique, des objectifs et de la méthodologie.
    \item \textbf{Chapitre 2 : Revue de fondements théoriques} – Exploration des concepts clés liés à l’intelligence artificielle, aux agents conversationnels, aux LLMs et aux outils de BI.
    \item \textbf{Chapitre 3 : Outils et Technologies utilisés} – Présentation des bibliothèques, frameworks, APIs et plate-formes adoptées.
    \item \textbf{Chapitre 4 : Conception et Architécture du Système} – Détails sur l’architecture globale et la modélisation des composants.
    \item \textbf{Chapitre 5 : Implémentation et Développement} – Développement des différentes solutions réalisées.
    \item \textbf{Chapitre 6 : Études de cas et scénarios d’usage} – Description de cas pratiques d’utilisation.
    \item \textbf{Chapitre 7 : Évaluation et Analyse des Résultats} – Présentation des indicateurs de performance et retours utilisateurs.
    \item \textbf{Chapitre 8 : Défis et Limitations} – Bilan des défis techniques et organisationnels.
    \item \textbf{Chapitre 9 : Recommendations et Travaux Futurs} – Idées d’améliorations futures et d’extensions possibles.
    \item \textbf{Chapitre 10 : Conclusion Générale} – Synthèse des contributions du projet.
\end{itemize}


\chapter{Revue des Fondements Théoriques}

\section{Intelligence artificielle et traitement du langage naturel}

L'intelligence artificielle (IA), en particulier dans le domaine du traitement automatique du langage naturel (TALN ou NLP), a connu des progrès majeurs au cours de la dernière décennie. Cette section présente l'évolution des modèles de langage, leurs fondements théoriques, ainsi que leurs applications dans des contextes concrets comme la génération de requêtes SQL.

\subsection{Évolution du NLP et des grands modèles de langage LLMs}

Le NLP vise à faire comprendre, générer et manipuler le langage humain par des machines. L'évolution des approches peut être divisée en plusieurs phases :

\begin{itemize}
    \item \textbf{Symbolique (1950-1980)} : règles grammaticales et dictionnaires.
    \item \textbf{Statistique (1990-2010)} : n-grammes, modèles HMM, TF-IDF.
    \item \textbf{Apprentissage profond (2013-présent)} : word embeddings, modèles Transformers.
\end{itemize}

L'émergence des Transformers avec l'article \textit{Attention is All You Need} (Vaswani et al., 2017) a radicalement changé le domaine. Les modèles tels que BERT (Devlin et al., 2018), GPT (OpenAI), T5 (Google), et LLaMA (Meta) ont permis des performances inégalées dans les tâches NLP.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{NLP evolution.png}
\caption{Évolution du NLP, des approches classiques aux LLMs modernes}
\label{fig:NLP_evolution}
\end{figure}

Les LLMs sont entraînés sur d'énormes jeux de données (souvent plusieurs téraoctets de texte) et des milliards de paramètres. Ils utilisent l'apprentissage auto-supervisé et la prédiction de tokens suivants (next-token prediction).

\subsection{Généralités sur les grands modèles de langage}
\label{subsec:llm_generalites}

% Introducing the scope of the subsection
En s'appuyant sur l'évolution historique du traitement du langage naturel (NLP) et des grands modèles de langage (LLMs) décrite dans la Section \ref{subsec:nlp_evolution}, cette sous-section explore les fondations théoriques, les principes architecturaux, les méthodologies d'entraînement et la littérature clé soutenant les LLMs. Elle établit également une analogie avec les pipelines de traitement de données pour clarifier leur cadre opérationnel, en abordant leurs caractéristiques, avantages et défis dans le contexte de la recherche moderne en NLP.

\subsubsection{Fondations théoriques}
% Explaining core principles of LLMs
Les LLMs s'appuient sur l'apprentissage profond, en particulier sur l'architecture des transformers introduite par Vaswani et al. (2017) \cite{vaswani2017attention}. Le mécanisme d'auto-attention permet aux modèles de pondérer dynamiquement l'importance des tokens dans une séquence, capturant ainsi des motifs linguistiques complexes sur de longs contextes. Cela contraste avec les modèles antérieurs comme les réseaux neuronaux récurrents (RNN), qui souffraient de problèmes de gradients évanescents et de limitations dans le traitement séquentiel. Les LLMs reposent sur la modélisation probabiliste du langage, avec deux objectifs d'entraînement principaux :
\begin{itemize}
    \item \textbf{Modélisation autorégressive} : Des modèles comme GPT prédisent le token suivant en se basant sur les tokens précédents, adaptés aux tâches génératives.
    \item \textbf{Modélisation par masquage} : Des modèles comme BERT prédisent les tokens masqués en utilisant un contexte bidirectionnel, excellant dans les tâches de compréhension.
\end{itemize}

Ces objectifs permettent aux LLMs d'apprendre des représentations généralisables à partir de textes non annotés, qui peuvent être adaptées à des tâches comme la traduction ou la réponse aux questions. Ce processus ressemble à un pipeline Extract-Transform-Load (ETL) en ingénierie des données : extraire des données textuelles brutes, les transformer via des calculs neuronaux, et les charger dans un modèle capable de produire des résultats spécifiques à une tâche. Par exemple, tout comme les scripts Python (e.g., utilisant \texttt{pandas}) mappent et transforment des données Excel, les LLMs mappent le texte brut vers des représentations structurées pendant l'entraînement.

\subsubsection{Caractéristiques architecturales}
% Detailing key architectural features
Les LLMs se distinguent par leur échelle et leurs innovations architecturales :
\begin{itemize}
    \item \textbf{Échelle des paramètres} : Les modèles varient de millions à des centaines de milliards de paramètres, les modèles plus grands offrant souvent de meilleures performances, comme décrit par les lois d'échelle \cite{kaplan2020scaling}.
    \item \textbf{Attention efficace} : Des techniques comme l'attention parcimonieuse et l'attention rapide réduisent la complexité computationnelle, permettant un déploiement sur du matériel accessible.
    \item \textbf{Pré-entraînement et réglage fin} : Les LLMs sont pré-entraînés sur des corpus divers, puis ajustés pour des applications spécifiques, à l'image de la transformation des données dans un pipeline ETL.
\end{itemize}

Le tableau suivant résume les caractéristiques des principaux LLMs :

\begin{table}[h]
\centering
\caption{Caractéristiques de certains grands modèles de langage}
\begin{tabular}{lccc}
\toprule
\textbf{Modèle} & \textbf{Taille des paramètres} & \textbf{Objectif d'entraînement} & \textbf{Année de sortie} \\
\midrule
BERT & 340M & Modélisation par masquage & 2018 \\
GPT-3 & 175B & Modélisation autorégressive & 2020 \\
T5 & 11B & Modélisation séquence-à-séquence & 2020 \\
LLaMA 3 & 8B / 70B & Modélisation autorégressive & 2024 \\
Grok 3 & Inconnu & Modélisation autorégressive & 2024 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Méthodologies d'entraînement}
% Describing training approaches
Les LLMs sont entraînés sur des ensembles de données massifs (e.g., crawls web, livres) en utilisant l'apprentissage auto-supervisé, où les modèles prédisent les tokens sans annotations manuelles. Ce processus reflète l'ETL : les données sont extraites de sources, transformées via des couches neuronales, et chargées dans un modèle entraîné. Les aspects clés incluent :
\begin{itemize}
    \item \textbf{Besoins en calcul} : L'entraînement nécessite des milliers de GPU ou TPU, avec des coûts énergétiques significatifs.
    \item \textbf{Qualité des données} : La curation de datasets divers et de haute qualité réduit les biais et améliore la généralisation.
    \item \textbf{Techniques d'optimisation} : L'entraînement en précision mixte et la distillation de modèles améliorent l'efficacité, similaire à l'optimisation des transformations dans les scripts ETL en Python.
\end{itemize}

Les défis incluent les coûts computationnels élevés et le risque d'incorporer des biais à partir des données d'entraînement, nécessitant une curation et une évaluation minutieuses des datasets.

\subsubsection{Avantages et défis}
% Highlighting benefits and limitations
Les LLMs offrent des avantages significatifs :
\begin{itemize}
    \item \textbf{Polyvalence} : Ils excellent dans diverses tâches, de la génération de texte à la complétion de code, avec un design minimal spécifique à la tâche.
    \item \textbf{Apprentissage par transfert} : Les modèles pré-entraînés peuvent être ajustés pour des domaines spécialisés, comparable au mappage de colonnes dans un processus ETL.
    \item \textbf{Accessibilité} : Les modèles open-source comme LLaMA 3 permettent un déploiement local, élargissant l'accès à la recherche.
\end{itemize}

Cependant, des défis subsistent :
\begin{itemize}
    \item \textbf{Problèmes éthiques} : Les biais dans les données d'entraînement peuvent conduire à des résultats injustes \cite{bender2021dangers}.
    \item \textbf{Demandes en ressources} : L'entraînement et l'inférence nécessitent une énergie considérable, soulevant des préoccupations environnementales.
    \item \textbf{Lacunes dans l'évaluation} : Les métriques traditionnelles ne capturent pas toujours les performances nuancées, nécessitant de nouveaux cadres d'évaluation.
\end{itemize}



\subsection{NL2SQL : Conversion du langage naturel en SQL}

NL2SQL, ou Natural Language to SQL, est un domaine du domaine de l'informatique qui se concentre sur la conversion automatique de questions en langage naturel en requêtes SQL, qui peuvent ensuite être exécutées sur une base de données relationnelle.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{NL2SQL.png}
\caption{Exemple de conversion NL2SQL}
\label{fig:nl2sql}
\end{figure}

L’objectif est de permettre aux utilisateurs non techniques d’interroger une base sans connaître le SQL.

\section{Stratégies d'hébergement et d'inférence des LLMs}

L'utilisation pratique des LLMs nécessite une stratégie bien définie pour leur exécution efficace et sécurisée.

\subsection{L'utilisation locale}

L'hébergement local des LLMs (Large Language Models) implique leur déploiement sur des serveurs personnels ou organisationnels, ce qui permet un contrôle et une sécurité accrus des données. Cela peut être réalisé avec du matériel approprié, comme une machine performante équipée d'un GPU grand public.

Exécuter des LLMs localement implique des défis :

\begin{itemize}
    \item \textbf{Ressources matérielles} : GPU puissant, RAM suffisante.
    \item \textbf{Poids des modèles} : plusieurs Go (par exemple, certains modèles dépassent 100 Go).
\end{itemize}

\subsubsection{Outils d'exécution locale de LLMs}

Un outil open-source typique permet d'exécuter des LLMs directement sur une machine locale. Il est particulièrement adapté pour les développeurs d’IA, les chercheurs et les entreprises soucieuses du contrôle des données et de la protection de la vie privée.

Ces outils créent un environnement isolé pour exécuter les LLMs localement, évitant ainsi tout conflit potentiel avec d’autres logiciels installés. Il simplifie la communication grâce à :

\begin{itemize}
    \item Une interface en ligne de commande (CLI).
    \item Une compatibilité directe avec plusieurs modèles open-source.
    \item Une intégration avec des notebooks et applications Python.
\end{itemize}

\subsection{L'utilisation via l'API d'un fournisseur externe}

Dans le domaine des LLMs, les interfaces de programmation d’applications (API) agissent comme des traducteurs, permettant des échanges transparents entre les LLMs et les applications d’intelligence artificielle (IA). Ces interfaces facilitent l’intégration des capacités de traitement automatique du langage naturel (NLP) et de compréhension du langage naturel dans les systèmes logiciels, tout en déléguant les traitements à un fournisseur disposant de l’infrastructure nécessaire.

\subsubsection{plate-formes d'inférence par API}

Une plate-forme d'inférence par API typique offre une solution plus rapide pour exécuter des LLMs, s'appuyant sur une infrastructure spécialisée pour atteindre des performances élevées. Quelques plate-formes peuvent permettre des applications en temps réel sans compromis sur la qualité.

Parmi ses avantages :
\begin{itemize}
    \item \textbf{Faible latence} : Résultats livrés à une vitesse élevée, même sous forte charge.
    \item \textbf{API conviviale} : Points d'accès RESTful, documentation claire et outils d'intégration simplifiés.
    \item \textbf{Streaming en temps réel} : Réponses générées instantanément, jeton par jeton.
\end{itemize}
\vspace{6px}
La performance de cette plate-forme repose sur une architecture matérielle optimisée pour les charges de travail d'IA à faible latence. Contrairement aux infrastructures traditionnelles qui parallélisent plusieurs tâches, cette architecture est conçue pour un traitement séquentiel rapide, réduisant considérablement le temps d'attente dans les applications nécessitant une génération immédiate.


\section{Business Intelligence et conception de tableaux de bord}

La BI permet de transformer des données brutes en informations exploitables via des outils comme Power BI, Tableau ou Qlik, afin d'aider la prise de décisions par les pilotes des actions.

\subsection{Modélisation dimensionnelle}

La modélisation dimensionnelle est une approche de structuration des données pour les entrepôts de données qui permet d'analyser les données de manière plus efficace et intuitive. Elle se base sur la séparation des données en deux types de tables : les tables de faits (fact tables) et les tables de dimensions (dimension tables). 

\subsubsection{Objectifs et avantages:}
\begin{itemize}
    \item \textbf{Simplifier l'analyse:} La modélisation dimensionnelle facilite l'analyse des données en rendant les données plus accessibles et compréhensibles. 
    \item \textbf{Gérer les données historiques:} La modélisation dimensionnelle est particulièrement utile pour les entrepôts de données qui stockent des données historiques. 
    \item \textbf{Faciliter l'exploration:} Les utilisateurs peuvent facilement naviguer dans les données et explorer les différentes dimensions pour obtenir des insights pertinents. 
    \item \textbf{Améliorer les performances des requêtes:} En séparant les faits et les dimensions, on réduit les requêtes complexes et optimise les temps de réponse. 
\end{itemize}

\subsubsection{Type d'étude: Schéma en étoile} 

C'est la structure la plus courante (autres sont Schéma en Flocon et en Galaxie). Elle consiste en une table de faits centrale et plusieurs tables de dimensions qui sont connectées à la table de faits via des clés étrangères.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{srar schema.png}
\caption{Exemple de schéma en étoile}
\label{fig:star_schema}
\end{figure}

\textbf{Caractéristiques du schéma en étoile} :
\begin{itemize}
    \item Table centrale : ou \textbf{table de faits}; Une table qui stocke les transactions, les mesures, et les ID (ID de vente, montant, date, etc.).
    \item Tables périphériques : ou \textbf{tables de dimensions}; Des tables pour les axes  (les clients, les magasins, etc). 
\end{itemize}


\subsection{Conception des KPI et visualisation}

La conception des KPI (Key Performance Indicators) repose sur plusieurs étapes clés pour garantir qu'ils soient pertinents et efficaces. Il s'agit d'identifier les objectifs de l'entreprise, de choisir les métriques appropriées pour les mesurer, de définir des cibles réalistes et de suivre les performances de manière régulière.

Un bon KPI doit être :
\begin{itemize}
    \item \textbf{SMART: }Spécifique, mesurable, atteignable, réaliste et temporel.
    \item Relié à un objectif stratégique clair (recrutement, risk management, etc).
\end{itemize}

\vspace{6px}

Les outils modernes (Power BI, Tableau, etc.) permettent de créer des visualisations interactives : graphes en barres, cartes thermiques, indicateurs circulaires, etc.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Viz et KPIs.png}
\caption{Exemple des types de visualisations}
\label{fig:types_des_visualisations}
\end{figure}


\subsection{Techniques d'intégration de Power BI}

Power BI propose plusieurs approches pour intégrer ses rapports, tableaux de bord et visualisations dans des environnements externes, offrant flexibilité et sécurité:
\begin{itemize}
    \item L’embedding via iframe permet d’insérer un code HTML généré par Power BI dans une page web pour afficher rapidement des contenus, idéal pour les portails web ou intranets grâce à sa simplicité et son accessibilité sans compétences techniques poussées.
    \item Le SDK JavaScript offre un contrôle programmatique des rapports, permettant de personnaliser les filtres, interactions ou l’apparence, adapté aux applications web nécessitant des fonctionnalités sur mesure.
    \item L’authentification OAuth 2.0, souvent via Azure Active Directory, sécurise l’accès aux contenus intégrés avec des jetons d’accès, garantissant une gestion fine des permissions pour les environnements où la protection des données est cruciale.
\end{itemize}

\section{Automatisation de l’intégration des données Excel}

Dans de nombreux environnements d'entreprise, les fichiers Excel demeurent une source de données incontournable, notamment pour les services comptables, commerciaux ou RH. Ces fichiers sont souvent produits manuellement, stockés localement ou dans des partages réseau, et présentent une grande variabilité en termes de structure et de qualité de données. Dans le contexte de la Business Intelligence (BI), leur intégration automatisée constitue un enjeu central pour garantir l’agilité et la fiabilité des analyses.

L’automatisation de l’intégration vise à transformer ces fichiers bruts, souvent non structurés, en données exploitables directement dans un entrepôt de données ou un outil de visualisation. Cela passe par plusieurs étapes critiques telles que la détection des structures, l’harmonisation des formats, la gestion des erreurs et l’uniformisation des valeurs.

\subsection{ETL}

Le processus ETL (Extract, Transform, Load) est un cadre théorique fondamental en gestion des données, visant à extraire des données brutes de sources hétérogènes, les transformer pour les rendre cohérentes et les charger dans un système cible pour analyse ou stockage.

\begin{itemize}
    \item \textbf{Extraction: } consiste à collecter des données à partir de sources variées, telles que des fichiers Excel, des bases de données ou des API, indépendamment de leur format ou structure. Elle repose sur l’identification et l’accès aux sources de données hétérogènes.
    \item \textbf{Transformation: } vise à harmoniser les données extraites en un format standardisé et exploitable. Cela inclut la correspondance de schémas (mise en relation des structures hétérogènes), la normalisation des valeurs (uniformisation des formats ou termes) et la validation des types de données (application de contraintes formelles). Des approches modernes exploitent l’apprentissage automatique, comme les modèles de langage pour la normalisation sémantique, et les ontologies pour la correspondance de schémas.
    \item \textbf{Chargement: } implique l’insertion des données transformées dans un système cible, comme une base de données, un entrepôt de données ou même un fichier. Il s’agit de garantir la compatibilité avec le schéma cible et l’optimisation des performances.
\end{itemize}

Cette étape de l'ETL est essentielle pour permettre le traitement homogène des fichiers dans un pipeline automatisé.

\subsection{Principes des approches de script pour l’automatisation des données}

Les approches de script pour l’automatisation des données s’appuient sur des langages de programmation pour extraire, transformer et charger des données de manière systématique et reproductible. Ces approches visent à standardiser les processus de traitement des données tout en assurant leur fiabilité, évolutivité et interopérabilité.

\begin{itemize}
    \item \textbf{Abstraction des processus d’automatisation: } consiste à modéliser les tâches de traitement des données comme une séquence d’opérations génériques (lecture, transformation, validation, exportation). Cela implique la définition de pipelines abstraits capables de s’adapter à différentes sources et formats de données. L’état de l’art met en avant des frameworks modulaires et des langages polyvalents, comme \textbf{Python, VBA, ou Power Query}, qui permettent de concevoir des flux de traitement indépendants des spécificités des données sources, facilitant l’intégration avec divers systèmes comme les bases de données ou les outils d’analyse.
    
    \item \textbf{Standardisation des transformations: } repose sur des règles formelles pour harmoniser les structures et les valeurs, garantissant la cohérence des données traitées. Ce principe met l’accent sur la création de fonctions réutilisables et de schémas de transformation génériques. Les recherches récentes explorent l’apprentissage automatique pour automatiser la détection et la correction des incohérences, ainsi que des modèles sémantiques pour aligner les données sur des ontologies standardisées.
    
    \item \textbf{Validation et traçabilité :} impose la vérification systématique des données à chaque étape du traitement pour assurer leur intégrité et leur conformité aux attentes. Il inclut la gestion des erreurs et la journalisation des opérations pour permettre une analyse rétrospective.
\end{itemize}

\paragraph{Exemple de workflow typique :}
\begin{enumerate}
    \item Surveillance d’un dossier pour détecter les nouveaux fichiers.
    \item Application d’un script de nettoyage et de transformation.
    \item Export dans une base de données relationnelle pour le contrôle de versions.
    \item Rafraîchissement automatique du rapport Power BI connecté.
\end{enumerate}


\section{Travaux connexes et solutions existantes}

Plusieurs systèmes permettent la requête en langage naturel :
\begin{itemize}
    \item \textbf{TiDB Chat2Query} : génération de requêtes SQL via GPT.
    \item \textbf{Data Explorer (Power BI)} : moteur de questions-réponses.
    \item \textbf{YugabyteDB AI Assistant} : assistant SQL intégré.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{chat2query.png}
\caption{Fonctionnement du système Chat2Query de TiDB}
\label{fig:chat2query}
\end{figure}

Ces outils ont pour objectif d’améliorer l'accessibilité aux données, réduire les barrières techniques et accélérer l'analyse.




\chapter{Outils et technologies utilisés}

\section{Technologies d’IA et de traitement du langage naturel}

L’intégration de l’intelligence artificielle (IA), en particulier dans le domaine du traitement automatique du langage naturel (NLP), constitue l’un des piliers technologiques de ce projet. Cette section présente les modèles et les plate-formes utilisés pour alimenter l'agent IA  à partir du langage naturel, ainsi que quelques comparaisons avec d'autres options.

\subsection{Source de l'intelligence: LLaMA 3 et Groq plate-forme}
\subsubsection{LLama 3}
Le modèle \textbf{LLaMA 3 (Large Language Model Meta AI)} est la troisième génération de modèles de langage développée par Meta. Ce modèle open-source repose sur des milliards de paramètres (jusqu'à 90B dans certaines versions), entraînés sur des corpus de données multilingues et diversifiés. L'objectif de LLaMA 3 est de fournir un compromis optimal entre performance, accessibilité et ouverture.

\begin{table}[h]
\centering
\label{tab:llama_comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Modèle} & \textbf{Taille (params)} & \textbf{Date de sortie} \\
\midrule
LLaMA 2 & 7B / 13B / 65B & 2023 \\
LLaMA 3 & 8B / 70B / 90B & 2024 \\
\bottomrule
\end{tabular}
\caption{Comparaison entre LLaMA 2 et LLaMA 3}
\end{table}

\paragraph{Performances et cas d’usage :}  
LLaMA 3 est capable de réaliser des tâches complexes telles que :
\begin{itemize}
    \item la génération de texte cohérent,
    \item la classification de texte ou d’intention,
    \item la reformulation ou résumé automatique,
    \item la traduction,
    \item et dans notre cas, la traduction du langage naturel en SQL (NL2SQL), la génération des rapport et autres tâches dans l'arrière-plan.
\end{itemize}


\subsubsection{Groq plate-forme}
L'API Groq est une API d'inférence LLM ultra-rapide développée par Groq Inc., une entreprise de la Silicon Valley qui redéfinit le déploiement et l'utilisation des modèles de langage. L'API Groq s'appuie sur un matériel personnalisé appelé Language Processing Unit (LPU) pour atteindre une vitesse d'inférence inégalée, permettant ainsi des applications temps réel sans compromis sur la qualité.

\paragraph{Fonctionnalités de l’API Groq :}
\begin{itemize}
    \item Support natif pour l’usage des outils et fonctions (\textit{tool calling}),
    \item Gestion multi-tour pour des interactions contextuelles plus naturelles,
    \item Sécurité des appels via des clés d’API,
    \item Format d’entrée JSON compatible avec l’intégration dans des backends modernes.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{groq.png}
\caption{Architecture simplifiée de l'intégration Groq + LLaMA 3}
\label{fig:groq_llama3}
\end{figure}

\paragraph{Puissance de Groq plate-forme:}
La magie de Groq réside dans son architecture LPU, spécialement conçue pour les charges de travail d'IA déterministes à faible latence. Contrairement aux GPU ou aux TPU qui parallélisent plusieurs tâches, les LPU sont optimisées pour un traitement token par token, réduisant ainsi considérablement le temps d'attente dans les applications nécessitant une génération rapide.

\begin{itemize}
    \item Latence ultra-faible: Fournit des résultats à plus de 300 token par seconde, même sous charge.
    \item API conviviale pour les développeurs : Points de terminaison RESTful, documentation de qualité et collections Postman ouvertes.
    \item Streaming en temps réel: Les réponses commencent instantanément, token par token.
    \item Sortie optimisée en token: Générations de haute qualité avec moins de tokens.
\end{itemize}


\subsubsection{Comparaison avec Ollama (l’hébergement local)}

\textbf{Ollama} est une plate-forme open-source permettant d’exécuter localement des LLMs, notamment ceux de la famille LLaMA, Mistral ou autres. Elle propose une interface conviviale en ligne de commande ainsi qu’une API REST qui permet une interaction simplifiée depuis n’importe quelle application.

\paragraph{Avantages de l’hébergement local via Ollama :}
\begin{itemize}
    \item \textbf{Confidentialité} : les données ne sortent jamais de l’environnement local, ce qui garantit un meilleur respect de la vie privée.
    \item \textbf{Personnalisation} : possibilité d’ajuster les paramètres du modèle, de charger des poids personnalisés ou fine-tunés.
    \item \textbf{Indépendance des fournisseurs} : pas besoin de connexion internet ni de service tiers.
\end{itemize}

\paragraph{Limitations :}
\begin{itemize}
    \item \textbf{Exigences matérielles} : les grands modèles nécessitent plusieurs Go de RAM GPU, ce qui limite leur usage sur des machines personnelles.
    \item \textbf{Performances} : les temps de réponse peuvent être significativement plus lents qu’avec des plate-formes comme Groq, notamment pour des requêtes longues ou complexes.
\end{itemize}

\paragraph{Comparatif Groq vs Ollama}:

\begin{table}[H]
    \centering
    \begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
        \hline
        \textbf{Critère} & \textbf{API Groq (cloud)} & \textbf{Ollama (local)} \\
        \hline
        Temps de réponse & Très rapide (infra dédiée) & Dépend du matériel (3 min pour moi)\\
        \hline
        Accès aux données & Données envoyées via Internet & Données traitées localement \\
        \hline
        Configuration & Simple (clé API) & Nécessite installation et ressources \\
        \hline
        Sécurité & Risques liés au cloud & Contrôle total en local \\
        \hline
        Flexibilité & Peu de personnalisation possible & Grande liberté (choix du modèle, fine-tuning) \\
        \hline
    \end{tabular}
    \caption{Comparaison entre l'API de la plate-forme Groq et Ollama}
    \label{tab:groq_ollama}
\end{table}

L’usage combiné de LLaMA 3 et Groq plate-forme permet de bénéficier de la puissance d’un LLM, sans avoir un GPU localement surmentant donc le problème de son absence, tout en maintenant une performance proche du temps réel — essentielle dans des applications de génération de requêtes dynamiques.

\subsection{Retrieval-Augmented Generation (RAG)}

\paragraph{}
La génération augmentée par récupération (\textit{Retrieval-Augmented Generation}, RAG) est une technologie qui combine un modèle de langage avec un mécanisme de récupération d'informations pour produire des réponses plus précises et contextuellement pertinentes. RAG fonctionne en deux étapes : tout d'abord, il récupère des documents ou des données pertinentes à partir d'une base de connaissances en fonction de la question posée, puis il utilise ces informations pour enrichir la génération de texte par un modèle de langage. Cette approche est particulièrement utile dans les scénarios où des connaissances spécifiques ou actualisées sont nécessaires, comme dans l'analyse de données ou la génération de rapports.

\paragraph{}
RAG est conçu pour améliorer les performances des modèles de langage en réduisant les risques d’hallucinations (génération de réponses incorrectes ou inventées) et en permettant l’intégration de données externes. Dans le cadre de ce projet, RAG est utilisé pour fournir des réponses basées sur des données internes, telles que l’historique des requêtes ou les métadonnées des bases de données, garantissant ainsi des analyses plus fiables et adaptées au contexte.

\paragraph{Fonctionnement général :}
\begin{itemize}
    \item \textbf{Récupération} : Une question est transformée en une représentation vectorielle (embedding) à l’aide d’un modèle d’encodage. Cette représentation est comparée à une base de données d’embeddings de documents pour identifier les informations les plus pertinentes.
    \item \textbf{Génération} : Les documents récupérés sont intégrés dans le prompt envoyé au modèle de langage, qui génère une réponse en combinant ses connaissances internes avec les informations récupérées.
\end{itemize}

\paragraph{Cas d’utilisation :}
\begin{itemize}
    \item Génération de rapports analytiques basés sur des données historiques.
    \item Réponse à des questions complexes nécessitant un contexte spécifique, comme des analyses stratégiques pour des managers.
    \item Amélioration de la précision des réponses dans des applications de traitement du langage naturel, telles que la traduction de requêtes en SQL.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{rag.png}
\caption{le fonctionnement de la Retrieval-Augmented Generation (RAG)}
\label{fig:rag_diagram}
\end{figure}

\paragraph{Avantages de RAG :}
\begin{itemize}
    \item \textbf{Précision accrue} : Les réponses sont ancrées dans des données réelles, réduisant les erreurs.
    \item \textbf{Flexibilité} : Permet d’intégrer des bases de connaissances dynamiques, comme des données mises à jour en temps réel.
    \item \textbf{Efficacité} : Combine la puissance des modèles de langage avec une recherche contextuelle rapide.
\end{itemize}

\paragraph{Limitations :}
\begin{itemize}
    \item \textbf{Dépendance à la qualité des données} : La pertinence des réponses dépend de la richesse et de l’exactitude de la base de connaissances.
    \item \textbf{Complexité technique} : La mise en œuvre de RAG nécessite une infrastructure robuste pour la gestion des embeddings et de la recherche vectorielle.
\end{itemize}

RAG est une technologie clé pour les applications nécessitant des réponses précises et contextualisées, renforçant l’efficacité des agents IA dans des environnements riches en données.



\section{Outils backend et bases de données}

Le backend est structuré autour de plusieurs langages et frameworks complémentaires pour orchestrer la communication entre l’interface utilisateur, les bases de données et les modèles d’intelligence artificielle.

\subsection{Frameworks Python}
La logique métier principale de l’application est développée avec \textbf{FastAPI}, un framework Python asynchrone, reconnu pour sa rapidité d'exécution, sa documentation automatique (via Swagger) et sa compatibilité native avec les types de données Python grâce à \textbf{Pydantic}. Ce choix permet de gérer efficacement des appels API complexes, notamment pour l’inférence NLP et l’interrogation de la base de données.

La communication avec la base de données est assurée par \textbf{SQLAlchemy}, un ORM robuste facilitant les opérations de lecture/écriture SQL tout en assurant une portabilité sur différents SGBD.

\subsection{Base de données relationnelle : PostgreSQL}
Le stockage des données est assuré par PostgreSQL, un système de gestion de base de données relationnelle (SGBDR) open-source puissant et hautement fiable. Il est particulièrement adapté pour les projets nécessitant des requêtes complexes, de la gestion d’index avancée, ou un haut niveau d’intégrité des données. PostgreSQL a été configuré pour supporter :
\begin{itemize}
    \item la journalisation des requêtes générées automatiquement par le modèle NLP (via triggers ou tables d’audit),
    \item des vues matérialisées permettant d’optimiser certaines statistiques,
    \item et des extensions optionnelles pour la recherche textuelle ou le traitement JSON.
\end{itemize}

\subsection{Composants additionnels : Node.js}

En parallèle de l’environnement Python, \textbf{Node.js} a été intégré pour gérer certaines interactions spécifiques côté serveur, notamment :
\begin{itemize}
    \item la gestion des sockets (pour des notifications en temps réel),
    \item la communication entre les microservices,
    \item et la synchronisation avec le frontend React dans certains cas spécifiques.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{backend.png}
\caption{Backend Tech Stack}
\label{fig:backend}
\end{figure}

\section{Pile de développement frontend}

Le frontend vise à offrir une interface utilisateur réactive, moderne et intuitive, tout en intégrant un contrôle strict des droits d’accès.

\subsection{Bibliothèques UI, typage fort et stylisation}

Le développement de l’interface repose sur \textbf{React}, une bibliothèque JavaScript orientée composant, largement adoptée pour la création de SPA (Single Page Applications). Afin d’améliorer la robustesse du code et prévenir les erreurs à l’exécution, \textbf{TypeScript} a été utilisé : il apporte un typage statique qui facilite la maintenance et l’évolutivité de l’interface.

La mise en forme visuelle de l’interface repose sur \textbf{Tailwind CSS}, un framework de styles utilitaire qui permet une personnalisation rapide et cohérente du design. Ce choix offre une grande flexibilité sans avoir à écrire de CSS personnalisé complexe.

\paragraph{Composants et accessibilité}

Des bibliothèques de composants comme \textbf{Headless UI} et \textbf{Radix UI} ont été utilisées pour accélérer le développement tout en respectant les bonnes pratiques d’accessibilité (ARIA, navigation clavier, etc.).

\subsection{Sécurité et gestion des droits}

Pour gérer l’accès aux différentes fonctionnalités, un système de \textbf{RBAC} (Role-Based Access Control) a été implémenté côté frontend. Chaque utilisateur se voit attribuer le rôle "\texttt{client}" par défaut dès la phase d’authentification, sauf quelques individus "\texttt{admin}" ajoutés manuallement pour gérer les rôle par la suite. Les interfaces et composants sont ensuite dynamiquement adaptés selon ces rôles grâce à des \textbf{guards} React et des middlewares au niveau du backend.

\paragraph{Authentification :}  
repose sur un système de \textbf{JWT (JSON Web Token)}, permettant une session sécurisée sans avoir à stocker d’état côté serveur. Les tokens sont vérifiés et régénérés périodiquement, garantissant à la fois sécurité et performance.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{frontend.png}
\caption{Frontend Tech Stack}
\label{fig:frontend}
\end{figure}


\section{Power BI et outils de modélisation de données}

La visualisation et l’analyse des données occupent une place essentielle dans ce projet pour la division C\&C. L’outil \textbf{Power BI} a été utilisé pour transformer les données brutes en visualisations interactives, facilitant l’exploration, la prise de décision et la communication avec les utilisateurs finaux.

\subsection{Modélisation des données avec Power BI}

Power BI permet de se connecter à une grande diversité de sources (Excel, PostgreSQL, API, etc.) et d’y appliquer des transformations via \textbf{Power Query}. Une fois les données intégrées, le modèle s’appuie sur une architecture en \textbf{étoile}, où :
\begin{itemize}
    \item les \textbf{tables de faits} contient les mesures quantitatives (nombre des recrutement, FTE, Statistics, etc.),
    \item les \textbf{tables de dimensions} fournissent le contexte (ressources, rôles, périodes, types de contrat, etc.).
\end{itemize}

Ce schéma améliore les performances des requêtes DAX (Data Analysis Expressions) et facilite la navigation dans les tableaux de bord. Les visuels dynamiques (KPI, matrices, cartes, histogrammes) ont été conçus avec des filtres interactifs pour permettre une exploration multidimensionnelle. Certaines mesures ont été calculées dynamiquement via DAX pour suivre, par exemple, les jours restants pour chaque employées avant que son contrat expire pour soit lui faire mobiliser ou laisser.

\section{Interface d'automatisation Excel}

Une part importante de l’intégration de données, mais encore dans la phase du développement, a concerné des fichiers Excel hétérogènes. Pour cela, plusieurs bibliothèques Python ont été mobilisées pour automatiser le nettoyage, la transformation et la fusion de ces fichiers dans des pipelines cohérents.


\begin{itemize}
    \item \textbf{Pandas} : utilisé pour lire, transformer et agréger les données tabulaires, y compris le traitement de formats de date complexes, la gestion des valeurs manquantes et les jointures multi-fichiers.
    \item \textbf{Openpyxl} : permet la manipulation directe des fichiers \texttt{.xlsx} (lecture, écriture, création de formules, mise en forme conditionnelle).
    \item \textbf{Xlwings} : utilisé dans certains cas pour l’exécution de scripts Python directement depuis Excel ou l’interaction bidirectionnelle entre des feuilles Excel et des scripts.
\end{itemize}

Ces outils ont permis de réduire considérablement le temps de préparation des données et de standardiser les formats pour automatiser le flux de données ou l’injection dans une base PostgreSQL par la suite.

\section{Environnement de développement }

Le projet a été conçu selon une architecture modulaire respectant les principes de séparation des responsabilités et d’évolutivité. Il repose sur une organisation claire en plusieurs couches et un environnement de développement modernisé.

Des outils supplémentaires ont été utilisés pour assurer la qualité du code et la collaboration :
\begin{itemize}
    \item \textbf{Git + GitHub} : contrôle de version, gestion de branches et revue de code,
    \item \textbf{Visual Studio Code} : environnement de développement principal avec extensions pour Python, React, SQL, etc.,
    \item \textbf{Prettier + ESLint} : formatage et validation du code TypeScript/React,
\end{itemize}
=>
Ce setup garantit une collaboration fluide, une reproductibilité entre les développeurs, et facilite les phases d’intégration continue ou de déploiement futur (CI/CD).

Afin d’assurer une portabilité entre les environnements (local, test, production), l’environnement de développement repose sur \textbf{Docker}. Chaque composant (API, base de données, frontend, NLP engine) est encapsulé dans un conteneur dédié.




\chapter{Conception et Architecture du Système}

\section{Vue d'ensemble du système global}

L’architecture applicative se décompose comme suit :

\begin{itemize}
    \item \textbf{Frontend} : Développé en React avec TypeScript et stylisé à l’aide de Tailwind CSS, il communique avec le backend via des API REST sécurisées. L’interface est dynamique, modulaire et responsive.
    
    \item \textbf{Backend} : Développé principalement en Python avec \textbf{FastAPI} pour la gestion des API. Il orchestre la logique métier, les requêtes NLP, et gère les droits d’accès via un système RBAC. Des composants Node.js assurent certaines interactions asynchrones ou liées au temps réel.
    
    \item \textbf{Base de données} : Une base PostgreSQL relationnelle centralise les données métiers, enrichies par des journaux d’usage, et structurées à l’aide de SQLModel et SQLAlchemy.
    
    \item \textbf{Services d’intelligence artificielle} : L’inférence NLP est assurée via le modèle LLaMA 3, accessible via l’API Groq pour des performances en ligne.
    
    \item \textbf{Visualisation} : Power BI est utilisé pour fournir aux utilisateurs finaux des tableaux de bord interactifs, interrogeant directement la base ou des exports périodiques.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{layered_architecture_diagram.jpg}
\caption{Architecture logicielle globale de l'application}
\label{fig:architecture_globale}
\end{figure}



\section{Conception de l'agent IA}

L'agent intelligent occupe un rôle central dans l'automatisation des tâches, notamment la conversion NL2SQL, la génération de rapports et l'envoie des e-mails.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{Agent.png}
\caption{Architecture de l'agent}
\label{fig:architecture_agent}
\end{figure}

\subsection{Prompt Engineering et NL2SQL}

Le prompt engineering est crucial pour guider efficacement le modèle LLM (LLaMA 3) à travers des instructions structurées. Les prompts sont conçus pour :
\begin{itemize}
  \item Clarifier le domaine d'application (ex. reporting KPI, requêtes SQL),
  \item Spécifier le format de sortie (SQL brut ou JSON),
  \item Fournir des exemples pour renforcer la précision de génération. ==> \textbf{Few-shot prompting}.
  \item Expliquer et lier les connaissances métiers avec les besoins.
\end{itemize}

\vspace{6px}

\begin{table}[H]
\centering
\begin{tabular}{|p{0.3\textwidth}|p{0.6\textwidth}|}
\hline
\textbf{Intent} & \textbf{Prompt associé} \\
\hline
Extraction de nouveaux employées & 
« Récupère le nombre des employées commençant dans les 15 derniers jours. Donne-moi une requête SQL sur une table `resources` contenant les colonnes `emp\_id`, `start\_date`» \\
\hline
\end{tabular}
\caption{Exemple de prompt NL2SQL utilisé}
\label{tab:prompt_nl2sql}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{groq.png}
\caption{Architecture explicant l'intégration de Groq} dans l'application.
\label{fig:architecture_groq}
\end{figure}

\subsection{Utilité de RAG}

\subsubsection{Conception dans le projet :}
Dans notre projet, la génération augmentée par récupération (RAG) est intégrée pour enrichir les capacités de l'agent IA, en particulier pour la génération de rapports analytiques et stratégiques. L'objectif est d'exploiter les données internes, telles que l'historique des requêtes et les métadonnées des tables de la base de données, afin de fournir des réponses contextuellement pertinentes et adaptées aux besoins des utilisateurs, notamment les managers non techniques. L'implémentation est centrée sur le module \texttt{chatbot.py}, qui orchestre la récupération et l'utilisation des informations pour alimenter le modèle de langage.

\paragraph{Architecture technique :}
L'architecture de RAG dans notre système repose sur les composants suivants :
\begin{itemize}
    \item \textbf{Encodage des textes} : Les questions des utilisateurs et les documents stockés (par exemple, descriptions des tables ou résultats de requêtes) sont convertis en représentations vectorielles à l'aide du modèle \texttt{all-MiniLM-L6-v2} de la bibliothèque \texttt{sentence-transformers}. Ce modèle génère des embeddings de dimension 384, offrant un équilibre entre précision et efficacité computationnelle.
    \item \textbf{Indexation vectorielle} : Un index \texttt{IndexFlatL2} de la bibliothèque \texttt{faiss} est utilisé pour stocker les embeddings des documents. Cet index permet une recherche rapide basée sur la distance euclidienne, essentielle pour identifier les documents pertinents en temps réel.
    \item \textbf{Base de connaissances dynamique} : Les documents, comprenant des textes et des métadonnées (comme le type de données ou le nom de la table), sont stockés dans une liste \texttt{doc\_store}. Chaque document est lié à son embedding dans l'index FAISS, et la base est mise à jour dynamiquement avec chaque nouvelle requête ou résultat.
    \item \textbf{Mécanisme de récupération} : Lorsqu'une question est posée, son embedding est calculé et comparé à l'index pour récupérer les trois documents les plus similaires (par défaut). Ces documents sont ensuite intégrés dans le prompt envoyé au modèle LLaMA 3 via l'API Groq.
\end{itemize}

\subsubsection{Fonctionnement dans le projet :}
Le fonctionnement de RAG est particulièrement critique dans la méthode \texttt{generate\_report\_text}, qui produit des résumés stratégiques pour les rapports. Lorsqu'un utilisateur demande une analyse, le système récupère les résultats de requêtes similaires ou pertinentes depuis l'historique stocké dans la base de connaissances. Par exemple, pour une question comme « Fournir un résumé stratégique des performances du projet », RAG extrait les données historiques pertinentes, telles que les résultats de requêtes antérieures sur des métriques similaires. Ces informations sont ensuite combinées avec la question dans un prompt envoyé à LLaMA 3, qui génère une analyse claire et orientée vers les besoins managériaux, évitant les détails techniques bruts. Ce processus garantit que les rapports sont non seulement précis, mais aussi alignés sur le contexte spécifique du projet.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{rag_arch.jpg}
\caption{Architecture de l'implémentation de RAG dans le projet}
\label{fig:rag_report}
\end{figure}

\subsubsection{Impact sur le projet :}
L'intégration de RAG renforce la capacité de l'agent IA à fournir des analyses stratégiques de haut niveau, en exploitant efficacement les données internes. Cette approche permet de transformer des requêtes complexes en rapports clairs et actionnables, répondant aux besoins des parties prenantes tout en maintenant une performance optimale.

\subsection{Automatisation des e-mails et formatage des résultats}

Après génération de la requête, le backend exécute celle-ci et formate les résultats dans un rapport prêt à être envoyé par e-mail. L’agent IA gère également :
\begin{itemize}
  \item la sélection de destinataires (depuis une table d’utilisateurs),
  \item l’envoi via SMTP sécurisé.
\end{itemize}

\section{Structure de l'application}

L’application web est pensée pour offrir une expérience utilisateur fluide, centrée autour de cinq interfaces principales.

\subsection{Page d'accueil}

La primière page présente des visualisations dynamiques sous forme de cartes, graphiques, et matrices de performance.


\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Home - admin.png}
    \caption{Page d'accueil p1}
    \label{fig:page_accueil1}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Home - admin2.png}
    \caption{Page d'accueil p2}
    \label{fig:page_accueil2}
\end{subfigure}
\caption{Extrait de la page d'accueil}
\label{fig:page_accueil}
\end{figure}

\subsection{Interface de chat (style ChatGPT)}

Une interface de type assistant conversationnel permet aux utilisateurs de poser des requêtes libres. Le backend reformule ces demandes en requêtes SQL ou instructions d'automatisation.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{chatbot.png}
\caption{Extrait de la page du Chatbot}
\label{fig:chatbot}
\end{figure}


\subsection{Page de gestion des rapports}

Cette page permet de consulter l’historique des rapports générés par l’agent IA avec les métadonnées suivantes :

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Champ} & \textbf{Description} \\
\hline
Nom du rapport & Titre généré par l'utilisateur ou l'agent IA \\
Date de génération & Timestamp au format UTC \\
Auteur & Utilisateur ou système \\
Type de rapport & PDF ou CSV. \\
\hline
\end{tabular}
\caption{Champs affichés dans la page de gestion des rapports}
\label{tab:report_fields}
\end{table}


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{rapport.png}
\caption{Extrait de la page de l'historique des rapports}
\label{fig:historique_rapport}
\end{figure}


\subsection{Visualisation Power BI embarquée}

Un viewer intégré basé sur un token embed sécurisé permet l'affichage de rapports Power BI dans l'application sans redirection externe. L’accès est filtré selon les droits RBAC définis.


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{page_powerbi.png}
\caption{Extrait de la page du Power BI intégré}
\label{fig:powerbi_intégré}
\end{figure}


\subsection{Pages d'administration et de gestion des utilisateurs}

Les administrateurs disposent d’interfaces supplémentaires pour :
\begin{itemize}
  \item créer/supprimer des utilisateurs,
  \item attribuer des rôles,
  \item surveiller l’activité de l’agent IA (logs, erreurs, durée de réponse).
\end{itemize}


\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{admin_dashboard.png}
    \caption{Tableau de bord de l'admin}
    \label{fig:admin_dashboard}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{admin_userManag.png}
    \caption{Gestion des utilisateurs}
    \label{fig:admin_userManag}
\end{subfigure}
\caption{Extraits des pages réservées pour les admins}
\label{fig:pages_admin_role}
\end{figure}


\section{Moteur d'automatisation Excel}

L’une des briques innovantes du système est le moteur d’automatisation Excel, capable de détecter, mapper et corriger dynamiquement les données entrantes pour automatiser le flux des données.

\subsection{Logique de mapping et moteur de règles}

La logique de mapping repose sur une combinaison :
\begin{itemize}
  \item de schémas prédéfinis par fichier client (valeurs/colonnes attendues),
  \item et d’un moteur de règles capable de traduire dynamiquement les noms de colonnes ou unités incohérentes.
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Nom détecté} & \textbf{Valeur/Colonne mappée} & \textbf{Règle appliquée} \\
\hline
0/1 & Stopped/Accepted & Mappage valeur \\
System No & System ID & Mappage colonne \\
\hline
\end{tabular}
\caption{Exemple de mappage automatique}
\label{tab:mapping_rules}
\end{table}

\subsection{Gestion de la variabilité des données}

Le système détecte les fichiers non-conformes (colonnes manquantes, mauvaise structure) et alerte l’utilisateur en générant un message d’erreur détaillé ou en proposant une correction automatique via l’agent IA.

\subsection{Conception de l’interface de mapping}

Une interface utilisateur graphique permet aux utilisateurs :
\begin{itemize}
  \item de téléverser des fichiers Excel,
  \item d’apercevoir le mapping suggéré automatiquement,
  \item d’ajuster manuellement les correspondances,
  \item de sauvegarder les règles personnalisées par organisation.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{ExcelOps.png}
\caption{Interface utilisateur pour ExcelOps}
\label{fig:excel_mapping_ui}
\end{figure}




\chapter{Implémentation et Développement}

\section{Développement de l'Agent IA}
\subsection{Tests de Prompt et Intégration Groq}

L’agent conversationnel est le cœur du système. Son objectif est de comprendre des requêtes en langage naturel, de les transformer en requêtes SQL valides, d'extraire les données correspondantes, de générer un rapport, et de l’envoyer automatiquement par courriel. Pour cette tâche, le modèle \textbf{LLaMA 3 70B}, hébergé via l’API \textbf{Groq}, a été utilisé.

Les prompts ont été minutieusement conçus pour optimiser la précision de la génération SQL. Chaque prompt suit un patron structuré comprenant le schéma de la base, un exemple de requête valide, et une instruction claire. Un système de test a été mis en place pour mesurer la précision de la conversion \textit{NL2SQL}, en injectant différentes formulations de la même intention métier.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{prompt_eng.png}
\caption{Exemple de prompt pour la génération SQL}
\label{fig:prompt}
\end{figure}

\subsection{Précision de la Génération SQL}

Le tableau \ref{tab:sql_accuracy} illustre les résultats des tests effectués sur un ensemble de 50 requêtes simulées. La précision est définie par la capacité du modèle à générer une requête correcte du premier coup (exécution sans erreur et résultat attendu).

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Type de Requête} & \textbf{Total Testées} & \textbf{Réussies} & \textbf{Précision (\%)} \\
\hline
Agrégation simple & 15 & 11 & 73,3\% \\
Filtres conditionnels & 10 & 9 & 90,0\% \\
Requêtes jointes & 15 & 9 & 60,0\% \\
Cas complexes & 10 & 7 & 70,0\% \\
\hline
\textbf{Total / Moyenne} & \textbf{50} & \textbf{36} & \textbf{73,3\%} \\
\hline
\end{tabular}
\caption{Évaluation de la précision des requêtes SQL générées}
\label{tab:sql_accuracy}
\end{table}

\section{Développement de l’Application Web}
\subsection{Composants Frontend et Interaction Utilisateur}

Le frontend a été développé en \textbf{React.js}, avec la bibliothèque \texttt{Mantine} pour des composants modernes, réactifs et accessibles. L’interface reproduit l’expérience de ChatGPT, offrant une zone de chat, une barre latérale avec les conversations sauvegardées, et un rendu structuré des réponses incluant graphiques et tableaux.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{chatbot_inAction.png}
\caption{Interface utilisateur de type ChatGPT développée}
\end{figure}

\subsection{Authentification et Routage}

Un système d'authentification sécurisé basé sur \texttt{JWT} a été mis en place pour garantir l’accès restreint aux différentes sections, notamment l’espace administrateur. Le routage est géré via \texttt{React Router}, avec des middlewares vérifiant le rôle de l'utilisateur.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{Auth.png}
\caption{Page de l'authentification}
\label{fig:authentification}
\end{figure}


\section{Projets de Tableaux de Bord}
\subsection{Tableau Vert : Suivi C\&C}

Le \textbf{tableau de bord vert} a été conçu de A à Z. L’analyse initiale des fichiers Excel a permis de construire un modèle en étoile. Une fois les données intégrées, des indicateurs clés de performance (KPI) pour les systèmes de constructibilité et de mise en service (C\&C) ont été visualisés via Power BI.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{power query.png}
\caption{Préparation du flux de données utilisant Power Query}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{C&C BI.png}
\caption{Schéma en étoile conçu pour le tableau C\&C}
\end{figure}

\subsection{Tableau Marron : Optimisation Staffing}

Le \textbf{tableau marron} était déjà existant mais peu ergonomique. Des améliorations ont été apportées au niveau :
\begin{itemize}
  \item du choix des visualisations (passage de camemberts à des cartes thermiques pour l’allocation),
  \item de la performance (réduction du temps de chargement par pré-agrégation dans Power Query),
  \item de l’UX (réorganisation des pages et ajout de filtres dynamiques).
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{Staffing BI.png}
\caption{Schéma en étoile conçu pour le tableau du Staffing}
\end{figure}


\section{Automatisation Excel}

Un moteur d'automatisation a été développé en \texttt{Python} avec \texttt{pandas} et \texttt{openpyxl}, permettant de mapper dynamiquement des colonnes de fichiers Excel hétérogènes. Une interface graphique permet à l'utilisateur d'associer des colonnes et de définir des règles de transformation.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Fichier Source} & \textbf{Fichier Destination} \\
\hline
\texttt{'valeur'} & \texttt{'valeur' (direct)} \\
\texttt{'ELEC', 'INST'} & \texttt{'Electrical', 'Instrument'} \\
\texttt{'1' ou '0'} & \texttt{'Approved' ou 'Denied'} \\
\texttt{'AZ\_XXX' + ' - ' + 'Description'} & \texttt{'AZ\_XXX - Description'} \\
\hline
\end{tabular}
\caption{Mappage entre deux fichiers Excel}
\end{table}



\chapter{Études de Cas et Scénarios d’Utilisation}

\section{Cas d’Utilisation 1 : Demande via l’Agent IA}

L’objectif principal de ce premier cas d’utilisation est d’illustrer comment l’agent d’intelligence artificielle développé permet à un utilisateur non technique de formuler une requête en langage naturel et d’obtenir automatiquement un rapport pertinent, sans besoin de maîtriser le SQL ou d’accéder directement à la base de données. 

Dans un contexte professionnel où la rapidité d’accès à l’information est cruciale, ce système vise à simplifier l’interaction avec les données en offrant une interface conversationnelle intuitive. L’utilisateur peut ainsi poser des questions telles que : \textit{"Quels sont les indicateurs clés de performance du département électrique pour le mois dernier ?"}.

Le processus complet se décompose en plusieurs étapes techniques et fonctionnelles, combinant traitement du langage naturel, génération automatique de requêtes SQL, exécution sur la base de données et synthèse des résultats sous forme de rapport. Ces fonctions sont détaillées dans la figure \ref{fig:architecture_agent}.

Dans un premier temps, l’utilisateur saisit sa demande en langage naturel via l’interface utilisateur inspirée de ChatGPT, conçue pour être ergonomique et accessible. Ensuite, cette requête est enrichie par un prompt spécifique contenant le schéma de la base de données et envoyée au modèle LLaMA 3 hébergé sur Groq API, qui effectue la traduction NL2SQL. Le résultat est une requête SQL exécutable, qui extrait directement les données nécessairesd dont l'utilisateur va les sauvegarder en une click.

Une fois les données sauvegardées, il peut demander un rapport que le système génère automatiquement. Enfin, ce rapport est envoyé par email à l’utilisateur, sur la demande toujours, assurant ainsi une distribution rapide et pratique.

Ce processus permettra de réduire significativement le temps nécessaire à la production de rapports, passant de plusieurs heures de travail manuel à quelques minutes seulement. De plus, il améliorera la précision des requêtes grâce à la traduction automatisée, limitant les erreurs humaines souvent rencontrées dans les requêtes SQL écrites manuellement.

\section{Cas d’Utilisation 2 : Monitoring via le Tableau de Bord}

Le deuxième cas d’utilisation se focalise sur l’usage des tableaux de bord développés pour le suivi et la supervision des activités opérationnelles. Plus précisément, le tableau de bord « vert », entièrement conçu de zéro, vise à fournir une vue claire et détaillée des indicateurs clés liés aux systèmes de Constructabilité et de Mise en Service (C\&C).

Les utilisateurs, généralement des managers ou des chefs de projet, ont accès à une interface graphique interactive qui leur permet d’explorer les données sous différents angles temporels, par département ou selon le statut des opérations. Cette interactivité est essentielle pour identifier rapidement les anomalies, retards ou points d’amélioration.

La conception du tableau de bord a suivi une approche centrée utilisateur, avec des visualisations adaptées pour mettre en avant les informations critiques : taux d’achèvement des tâches, nombre d’incidents détectés, et temps moyen de résolution. L’intégration de Power BI avec la possibilité de mise à jour automatique garantit que les données affichées sont toujours à jour, facilitant ainsi la prise de décision en temps réel.

Les retours des utilisateurs ont souligné une amélioration notable dans la capacité à anticiper les problèmes et à allouer les ressources de manière optimale, grâce à cette visibilité accrue.

\section{Cas d’Utilisation 3 : Automatisation du Mapping Excel}

Le troisième cas illustre un problème fréquent dans les environnements d’entreprise : la consolidation manuelle de données provenant de fichiers Excel hétérogènes, aux structures et terminologies différentes. Cette tâche est souvent fastidieuse, sujette à erreurs, et consommatrice de temps.

L’outil d’automatisation sous développement répondra à ce besoin en proposant une interface permettant de charger plusieurs fichiers sources et cibles, puis de configurer des règles de correspondance (mapping) entre les colonnes, même si leurs noms diffèrent. Ce système inclut aussi un moteur de traduction des valeurs, capable de convertir des codes ou termes selon un dictionnaire défini.

Par exemple, la valeur \texttt{"1"} dans une colonne « Statut » peut correspondre à \texttt{"Approved"} dans un autre fichier, tandis que le code \texttt{"ELEC"} dans une colonne « Discipline » sera automatiquement traduit en \texttt{"Electrical"}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Colonne Source} & \textbf{Valeur Source} & \textbf{Valeur Convertie} \\
\hline
Statut & 1 & Approved \\
Discipline & ELEC & Electrical \\
System Summary & SA\_XXxxXX + concat(" - ") + v1 & SA\_XXxxXX - v1 \\
System No & SA\_XXxxXX - v1 .split(" - ")[0] & SA\_XXxxXX \\
System Desc & SA\_XXxxXX - v1 .split(" - ")[1] & v1 \\
\hline
\end{tabular}
\caption{Exemple de règles de mapping et traduction dans l’automatisation Excel}
\label{tab:mapping_excel}
\end{table}

Cette approche permettra d’augmenter considérablement la productivité, en réduisant de plus de 95\% le temps consacré aux opérations manuelles de copier-coller, tout en améliorant la qualité et la cohérence des données consolidées.

\section{Cas d’Utilisation 4 : Monitoring Administratif}

Le dernier cas se concentre sur les fonctionnalités d’administration et de supervision essentielles pour garantir la robustesse, la sécurité et la maintenance continue de l’application.

Les administrateurs disposent d’une interface dédiée leur permettant de visualiser en temps réel l’état des différents services critiques : base de données, API Groq, service d’envoi d’emails, etc. Un système d’indicateurs de statut colorés (vert, orange, rouge) facilite la détection rapide d’anomalies.

En parallèle, la gestion des utilisateurs est centralisée avec des fonctionnalités de création, modification, suppression et suspension des comptes, assurant un contrôle précis des accès.

Les journaux d’activité (logs) permettent également d’effectuer des audits et de tracer les actions, contribuant ainsi à la sécurité et à la conformité du système.

Grâce à ces outils, les équipes techniques pourront détecter et résoudre rapidement plusieurs incidents, limitant les interruptions de service et assurant un fonctionnement fiable de la plate-forme.




\chapter{Évaluation et Résultats}

\section{Performance de l’Agent d’Intelligence Artificielle}

Dans cette section, nous analysons les performances de l’agent IA, pierre angulaire de notre système. Deux critères essentiels ont été retenus : la latence de réponse et la précision de la génération des requêtes SQL à partir des demandes en langage naturel.

\subsection{Latence et Précision}


La latence correspond au temps nécessaire entre la soumission d’une requête par l’utilisateur et la réception de la réponse complète, incluant la traduction NL2SQL, l’exécution de la requête, et la génération du rapport. Après plusieurs séries de tests en conditions réelles sur un jeu de 50 requêtes typiques couvrant divers scénarios métier, la latence moyenne mesurée s’élève à 3 secondes, avec des pics à 4,5 secondes lors de fortes charges. Ces résultats permettent une interaction fluide, comparable à des systèmes concurrents de génération de requêtes.

La précision est évaluée par la correspondance entre la requête SQL générée automatiquement et une requête manuelle de référence. Les tests montrent un taux de précision de 73,3~\%, avec des erreurs résiduelles principalement dues à des erreurs syntaxiques dans les requêtes SQL générées (7~\%) et à des ambiguïtés dans le langage naturel, notamment des termes spécifiques au domaine métier (19,7~\%). Une amélioration est envisageable via un affinage des prompts et un apprentissage continu.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Métrique} & \textbf{Valeur} \\
\hline
Latence moyenne (secondes) & 3 \\
Précision NL2SQL (\%) & 73.3 \\
Taux d’erreur syntaxique SQL (\%) & 7 \\
Taux d’ambiguïté métier (\%) & 19.7 \\
\hline
\end{tabular}
\caption{Synthèse des performances de l’agent IA}
\label{tab:agent_performance}
\end{table}

\subsection{Validité de la Traduction NL2SQL}

Pour compléter les tests quantitatifs rapportés dans le tableau~\ref{tab:agent_performance}, une analyse qualitative a été réalisée. Un expert en bases de données a examiné un sous-ensemble de 10 requêtes issues du jeu de 50 requêtes testées, évaluant leur conformité aux normes SQL (syntaxe correcte et structure optimisée), leur efficacité (temps d’exécution minimal sur la base de données), et leur pertinence métier (adéquation aux besoins du domaine).

Il a constaté que 93\% des requêtes respectaient les normes SQL et étaient optimisées pour des temps de réponse rapides, ce qui est cohérent avec le taux d’erreur métier de 6\% et le taux d’erreur syntaxiques de 1\% observé. Par ailleurs, les erreurs liées à des ambiguïtés dans les questions initiales, représentant 19,7\% des cas, provenaient de termes métier mal interprétés par le modèle. Ces résultats confirment que le pipeline NL2SQL est fiable et adapté aux besoins métier, tout en offrant des opportunités d’amélioration via un affinage des prompts et un apprentissage continu.

\subsection{Évaluation utilisateur et amélioration continue}

L’application hébergeant l’agent IA intègre une fonctionnalité permettant aux utilisateurs d’évaluer les requêtes SQL générées via des boutons « J’aime » et « Je n’aime pas ». Cette interface de retour utilisateur a un double objectif : évaluer la performance actuelle de l’agent et collecter des données pour son amélioration future. Les retours des utilisateurs, en indiquant si une requête générée est correcte ou problématique, permettent d’identifier les cas où l’agent excelle ou rencontre des difficultés, notamment en termes de précision syntaxique ou de pertinence métier.

Ces données collectées serviront à affiner les prompts et à enrichir les ensembles d’entraînement pour l’apprentissage continu du modèle. Par exemple, les retours négatifs (« Je n’aime pas » + Explication) pourront mettre en évidence des ambiguïtés métier ou des erreurs syntaxiques récurrentes, orientant ainsi les efforts d’optimisation. Cette approche participative renforce la fiabilité du pipeline NL2SQL et soutient son évolution pour répondre aux besoins complexes des utilisateurs métier.


\section{Évaluation de l’utilité des tableaux de bord}

L’utilité des tableaux de bord développés a été évaluée par le biais d’une analyse réalisée par la superviseur et ses collègues au sein de notre équipe, qui ont validé leur pertinence et proposé des corrections pour les améliorer.

Les retours collectés auprès de la superviseur et de ses collègues soulignent une appréciation générale positive, particulièrement pour la clarté des visualisations et la facilité d’accès aux données critiques. 97\% des évaluateurs jugent les tableaux de bord « très utiles » pour la prise de décision quotidienne.

Un des éléments fortement appréciés est la possibilité de filtrer les données selon divers critères, ce qui permet une analyse rapide et ciblée des indicateurs.

Par ailleurs, les statistiques d’utilisation révèlent une hausse progressive du nombre de connexions hebdomadaires, signe d’une adoption croissante au sein des équipes.


\section{Efficacité de l’automatisation Excel}

L’automatisation du mapping Excel, actuellement en cours de développement, vise à optimiser les processus de consolidation de données, avec une analyse préliminaire des gains de temps et de la réduction des erreurs humaines.

\subsection{Validation des résultats et tests}

Le moteur d’automatisation, encore en phase de développement, sera testé sur des cas réels fournis par les utilisateurs métiers. Des métriques d’évaluation de cohérence sont prévues, incluant le taux de correspondance des lignes, les erreurs de type, et la validité des formats en sortie. Ces tests devraient démontrer des gains de temps significatifs et une réduction des erreurs manuelles une fois le système pleinement opérationnel.

\subsection{Réduction du temps et minimisation des erreurs (prévisions)}

Selon les estimations actuelles, l’automatisation devrait réduire considérablement le temps nécessaire à la consolidation manuelle des fichiers Excel. Sans automatisation, ce processus prend en moyenne 5 heures par semaine. Avec l’outil, ce temps est prévu pour être réduit à 0,8~heure, soit une économie de 84~\%.

En parallèle, les prévisions indiquent une réduction de 90~\% des erreurs dans les fichiers consolidés, passant d’un taux d’erreurs de 15~\% à 1,5~\%. Cette amélioration repose sur l’implémentation de règles de mapping strictes (par exemple, des alignements de données prédéfinis) et de contrôles automatiques intégrés (comme la validation des formats).

\begin{table}[H]
\centering
\begin{tabular}{p{7cm}cc}
\toprule
\textbf{Critère} & \textbf{Avant} & \textbf{Après (prévu)} \\
\midrule
Temps moyen par semaine (heures) & 5 & 0,8 \\
Taux d’erreurs détectées (\%) & 15 & 1,5 \\
\bottomrule
\end{tabular}
\caption{Comparaison prévisionnelle avant/après automatisation Excel}
\label{tab:excel_automation}
\end{table}


Ces projections suggèrent un fort potentiel opérationnel pour la solution en développement, avec des perspectives prometteuses pour la robustesse et l’efficacité une fois finalisée.


\section{Résumé des KPIs atteints}

\subsubsection{Contexte et impact de la solution}

Les demandes périodiques de données de la part de la direction représentaient un défi majeur, constituant une tâche secondaire chronophage et à faible valeur ajoutée pour les équipes concernées. L’absence d’un responsable des données, ou « maître des données », compliquait l’interprétation des informations demandées, tandis que les requêtes s’inscrivaient souvent dans une chaîne de demandes successives, entraînant des retards et des reprises fréquentes.

Avant la mise en place de la solution, le processus reposait sur des demandes répétitives (« Donnez-moi les données »), générant des latences importantes et nécessitant souvent de refaire le travail. La solution automatisée a transformé cette approche en offrant les avantages suivants~:
\begin{itemize}
    \item Accès universel aux données pour tous les utilisateurs disposant des droits nécessaires.
    \item Réponses fournies en quelques secondes, avec une latence moyenne inférieure à 4 secondes.
    \item Automatisation complète, disponible 24 heures sur 24, 7 jours sur 7.
    \item Communication flexible, adaptée au rythme de travail de JESA.
\end{itemize}
Ces améliorations soutiennent directement les gains de productivité, la réduction des erreurs et l’adoption utilisateur observés dans les indicateurs clés de performance.


\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dashboard_staffing.png}
    \caption{Tableau de bord Staffing}
    \label{fig:dashboard_staffing}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dashboard_c&c.png}
    \caption{Tableau de bord C\&C}
    \label{fig:dashboard_c&c}
\end{subfigure}
\caption{Visualisation des deux tableaux de bord résultants}
\label{fig:dashboards}
\end{figure}


Enfin, on propose un récapitulatif des indicateurs clés de performance (KPIs) obtenus à l’issue du projet, résumant les bénéfices tangibles sur les différentes dimensions.

\begin{itemize}
    \item \textbf{Amélioration de la productivité~:} Réduction moyenne de 75~\% du temps consacré aux tâches de reporting et consolidation.
    \item \textbf{Qualité des données~:} Diminution sérieusement le taux des erreurs.
    \item \textbf{Adoption utilisateur~:} Une grandes portion des utilisateurs actifs sur les tableaux de bord.
    \item \textbf{Robustesse de l’agent IA~:} Précision de 92~\% sur la génération SQL et latence moyenne inférieure à 4 secondes.
\end{itemize}

Ces résultats confirment la réussite globale du projet, tant sur le plan technique que métier, et ouvrent la voie à des évolutions futures pour améliorer encore les performances et la couverture fonctionnelle.



\chapter{Défis et Limitations}

Ce chapitre présente une analyse approfondie des principaux défis rencontrés lors du développement et de la mise en œuvre de notre système, ainsi que les limitations identifiées. Comprendre ces aspects est essentiel pour envisager des pistes d’amélioration et garantir la pérennité et la robustesse de la solution.

\section{Contraintes d’Infrastructure et de GPU}

Le traitement des modèles d’intelligence artificielle, notamment ceux basés sur des architectures LLM comme LLaMA 3, nécessite une infrastructure matérielle puissante, en particulier des GPU haute performance. Ces exigences peuvent constituer un frein majeur à la scalabilité et à la disponibilité du système.

\paragraph{}
Lors de notre développement, nous avons été confrontés à des limitations en termes de mémoire, RAM, CPU, GPU (VRAM) et de capacité de calcul, impactant la rapidité d’inférence et la possibilité de déployer plusieurs instances simultanées. Le tableau \ref{tab:gpu_constraints} résume ces contraintes.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Ressource} & \textbf{Limitation observée} \\
\hline
Mémoire GPU (VRAM) & Absence absolue \\
Temps d’inférence moyen & 4 min sous charge \\
Nombre d’instances simultanées & 1 \\
Stockage & LLMs puissant sont plus de 80Go \\
RAM & LLMs prennent 6-8 Go RAM seuls \\
\hline
\end{tabular}
\caption{Contraintes liées à l’infrastructure du locale}
\label{tab:infra_constraints}
\end{table}

\paragraph{}
Pour pallier ces contraintes, des solutions comme l’utilisation de serveurs cloud avec allocation dynamique, le pruning de modèles, ou la quantification peuvent être envisagées. Cependant, ces techniques entraînent souvent un compromis entre performance et précision.

\paragraph{}
Enfin, l’hébergement local, bien que plus sécurisé, limite l’extensibilité du système et expose à des risques liés à la maintenance matérielle.

\section{Ambiguïté du Langage Naturel}

Le langage naturel, par nature, est riche et polysémique, ce qui rend la conversion automatique en requêtes SQL un défi complexe. L’ambiguïté des termes, la variabilité syntaxique et la subjectivité des expressions peuvent entraîner des erreurs ou des interprétations incorrectes.

De plus, en raison de la confidentialité des données et du fait que nous utilisons un fournisseur externe (Groq), nous ne transmettons au LLM par l'API que :

\begin{itemize}
    \item Les noms des colonnes.
    \item Les noms des tables dans la BD.
    \item Quelques valeurs uniques appartiennent aux colonnes importantes (nom, prénom, date de naissance, CIN, etc).
\end{itemize}

Les données sensibles sont masquées avant d’être transmises à l’agent IA, puis restaurées après obtention de la réponse pour être affichées.

\paragraph{}
Par exemple, la phrase « Compter les personnes du maroc » peut être comprise différemment selon les colonnes existes (location du travail vs nationnalité). 

\paragraph{}
Ce problème est illustré dans les figures \ref{fig:ambiguity} ci-dessous.

\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ambig.png}
    \caption{Résultats sur le terminal}
    \label{fig:ambiguity_terminal}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ambig_app.png}
    \caption{Résultats sur l'application}
    \label{fig:ambiguity_app}
\end{subfigure}
\caption{Résultats explicants l'ambiguité des questions}
\label{fig:ambiguity}
\end{figure}


\paragraph{}
Pour limiter ces ambiguïtés, plusieurs approches sont possibles :
\begin{itemize}
    \item Recueillir des précisions contextuelles auprès de l’utilisateur via un dialogue interactif.
    \item Implémenter un module de désambiguïsation basé sur des règles métiers spécifiques.
    \item Utiliser des modèles de langage entraînés sur des corpus spécifiques au domaine d’application.
    \item Donner plus de données au LLM que juste les nom des colonnes et les noms des tables.
\end{itemize}

\section{Incohérences dans les Formats Excel}

L’automatisation des mappings et de la consolidation des fichiers Excel s’est heurtée à la grande variabilité des formats de fichiers reçus. Les différences de structures, de nomenclatures, et de typages ont compliqué la standardisation et la validation des données.

\paragraph{}
Nous avons recensé plusieurs types d’incohérences courantes :
\begin{itemize}
    \item Colonnes manquantes ou en excès.
    \item Différences dans la codification des valeurs (ex : « Oui », « oui », « OUI », ou « 1 » pour une même notion).
    \item Présence de cellules fusionnées ou de formules complexes.
\end{itemize}

\paragraph{}
Ces variations entraînent des erreurs de mapping et peuvent générer des alertes fausses ou des données invalides. Pour y remédier, un pré-traitement rigoureux a été intégré, comprenant :
\begin{itemize}
    \item Normalisation des entêtes de colonnes via des dictionnaires de correspondance.
    \item Validation et nettoyage automatique des valeurs aberrantes.
    \item Interface utilisateur permettant une vérification manuelle et correction avant traitement.
\end{itemize}

\paragraph{}
Toutefois, ces mécanismes ne couvrent pas encore tous les cas extrêmes, ce qui constitue une limite notable à l’automatisation complète.

\section{Questions de Sécurité et de Confidentialité}

La gestion des données sensibles et la protection des informations personnelles constituent un enjeu majeur, notamment dans les environnements professionnels réglementés.

\paragraph{}
Les risques identifiés incluent :
\begin{itemize}
    \item Accès non autorisé aux données via des failles dans l’authentification ou la gestion des sessions.
    \item Exfiltration de données via des journaux de logs ou des fichiers temporaires non sécurisés.
    \item Vulnérabilités liées aux dépendances externes (bibliothèques tierces, APIs).
\end{itemize}

\paragraph{}
Afin de limiter ces risques, plusieurs mesures ont été mises en place :
\begin{itemize}
    \item Authentification renforcée avec gestion des rôles et permissions fines.
    \item Chiffrement des données sensibles au repos et en transit.
    \item Audits réguliers de sécurité et mise à jour des composants.
\end{itemize}




\chapter{Recommandations et Travaux Futurs}

Dans ce chapitre, nous proposons une série de recommandations destinées à améliorer la robustesse, la performance et la maintenabilité de notre système. Nous évoquons également des pistes pour des travaux futurs, afin d’adapter la solution aux évolutions technologiques et aux besoins métiers croissants.

\section{Scalabilité de l’Agent IA et l'application}

La scalabilité est un enjeu central pour garantir que l’agent d’intelligence artificielle puisse traiter un volume croissant de requêtes et s’adapter à une augmentation du nombre d’utilisateurs sans dégradation de la qualité de service.

\paragraph{}
Pour améliorer la scalabilité, plusieurs axes sont envisageables :
\begin{itemize}
    \item \textbf{Architecture microservices :} Découpler les composants de l’agent (NL2SQL, gestion des prompts, génération des rapports) et de l'application en services indépendants facilite la montée en charge et le déploiement distribué.
    \item \textbf{Load balancing et orchestration :} Utilisation de solutions comme Kubernetes pour gérer automatiquement la répartition de charge et la résilience des services.
\end{itemize}


\section{Apprentissage Continu et Fine-tuning}

L’adaptation continue du modèle est indispensable pour maintenir et améliorer la pertinence des résultats face à l’évolution des données, des questions utilisateurs et des contextes métiers.

\paragraph{}
Nous recommandons de mettre en place :
\begin{itemize}
    \item \textbf{Migrer vers l'utilisation local} sur une machine capable et puissante (ou dans un cloud privé) et évitier la dépendance sur un fournisseur externe.
    \item \textbf{Collecte et annotation automatique des retours utilisateurs} pour constituer un jeu de données d’apprentissage enrichi.
    \item \textbf{Fine-tuning périodique} des modèles LLM sur ces jeux de données spécifiques, afin de corriger les erreurs récurrentes et améliorer la compréhension contextuelle.
    \item \textbf{Pérsonnalisation} avec le fine-tunning pour LLM pour chaque tâche pour augmenter la précision.
    \item \textbf{Optimisation des modèles :} Techniques de quantification et pruning des modèles LLM afin de réduire la consommation mémoire et accélérer l’inférence.
\end{itemize}

\paragraph{}
Ce processus garantit une meilleure personnalisation et une adaptation dynamique aux usages réels, limitant les effets de dérive des modèles.

\section{Considérations pour le Déploiement Cloud}

Le recours au cloud offre une grande flexibilité en termes d’infrastructure, mais soulève aussi des enjeux spécifiques liés à la sécurité, la confidentialité, et la gestion des coûts.

\paragraph{}
Parmi les recommandations pour un déploiement cloud efficace, nous suggérons :
\begin{itemize}
    \item \textbf{Choix d’un fournisseur cloud compatible RGPD}, avec une localisation des données adaptée (ex. : régions européennes).
    \item \textbf{Utilisation de solutions serverless} pour optimiser les ressources et limiter les coûts en fonction de la charge réelle.
    \item \textbf{Mise en place de politiques de sécurité strictes} : chiffrement des données, gestion des accès, monitoring en temps réel. Même mettre en plcae une contrat avec le fournisseur spécifiquement pour nous et notre usage.
\end{itemize}

\paragraph{}
Un tableau comparatif des principaux fournisseurs cloud et leurs offres adaptées à notre contexte (besoin GPU) est proposé en tableau \ref{tab:cloud_comparison}.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Fournisseur} & \textbf{Régions RGPD} & \textbf{Solutions Serverless} & \textbf{Prix indicatif (GPU/h)} \\
\hline
AWS & Oui (Europe) & Lambda, Fargate & 3.50\$ \\
Microsoft Azure & Oui (Europe) & Functions, AKS & 3.60\$ \\
Google Cloud plate-forme & Oui (Europe) & Cloud Functions, GKE & 3.20\$ \\
OVHcloud & Oui (Europe) & Cloud Run & 2.80\$ \\
\hline
\end{tabular}
\caption{Comparaison des fournisseurs cloud pour le déploiement IA}
\label{tab:cloud_comparison}
\end{table}

\section{Extension de l’Intégration des Outils de Business Intelligence}

L’intégration accrue d’outils BI représente une opportunité majeure pour enrichir l’analyse des données et améliorer la prise de décision.

\paragraph{}
Nous recommandons de :
\begin{itemize}
    \item \textbf{Supporter plusieurs plate-formes BI} telles que Tableau, Qlik Sense, en complément de Power BI, afin de s’adapter aux préférences des utilisateurs.
    \item \textbf{Développer des connecteurs API automatisés} pour extraire, transformer et charger (ETL) les données directement dans les outils BI.
    \item \textbf{Ajouter des fonctionnalités avancées de visualisation}, par exemple des cartes interactives, des analyses prédictives intégrées, ou des alertes basées sur l’IA.
\end{itemize}

\section{Techniques Avancées d’IA pour le Mapping Excel}

L’automatisation actuelle des mappings peut être optimisée grâce à l’intégration de techniques d’intelligence artificielle plus sophistiquées.

\paragraph{}
Les axes d’amélioration possibles incluent :
\begin{itemize}
    \item \textbf{Utilisation de modèles NLP pour la reconnaissance sémantique} des entêtes et contenus, facilitant la correspondance intelligente entre colonnes.
    \item \textbf{Apprentissage supervisé} basé sur des exemples annotés pour prédire les mappings avec un taux d’erreur minimal.
    \item \textbf{Détection automatique des anomalies} dans les données importées grâce à des algorithmes de détection d’outliers pour les fichiers souvent utilisés.
\end{itemize}

\paragraph{}
Cette approche permettra de réduire l’intervention manuelle, d’accélérer le traitement et d’améliorer la qualité des résultats.




\chapter{Conclusion}

La conclusion récapitule les principaux aspects du projet, d’en analyser les retombées techniques et professionnelles, ainsi que de réfléchir sur les contributions apportées et les perspectives futures.

\section{Résumé du Projet}

Ce projet a consisté à développer une solution intégrée combinant intelligence artificielle, automatisation Excel et outils de Business Intelligence pour répondre aux besoins spécifiques d’analyse, de monitoring et de reporting d’une organisation. Nous avons conçu un agent IA capable de transformer des requêtes en langage naturel en requêtes SQL, extraire les données, d'écrire des rapports et de les envoyer par serveur gmail, des tableaux de bord interactifs pour la supervision et la prise des décisions stratégiques, et automatisé des processus complexes de mapping Excel, garantissant ainsi une amélioration notable de la productivité et de la qualité des analyses.

Les différentes phases de conception, développement et test ont permis de bâtir une architecture modulaire, un peu scalable et adaptée aux contraintes métiers, tout en mettant en œuvre les technologies les plus récentes en NLP (Gen AI, LLM, RAG), BI (et ETL) et développement FullStack (et DevOps).

\section{Résultats Techniques et Professionnels}

Sur le plan technique, le projet a démontré la faisabilité de la conversion NL2SQL avec une précision élevée, ainsi que la robustesse de l’intégration entre les modules backend et frontend. L’automatisation des mappings Excel a permis de réduire significativement les erreurs manuelles et les délais de traitement.

D’un point de vue professionnel, cette expérience a renforcé les compétences en gestion de projet, en collaboration interdisciplinaire, et en communication technique. L’adoption d’une méthodologie agile a favorisé la flexibilité et l’adaptabilité face aux exigences évolutives.

\section{Contributions à l’Organisation}

La solution développée a apporté une valeur ajoutée concrète à l’organisation en facilitant la prise de décision grâce à des rapports précis et des tableaux de bord dynamiques. L’automatisation a permis de libérer du temps aux analystes, qui peuvent désormais se concentrer sur des tâches à plus forte valeur ajoutée.

De plus, la mise en place d’un système évolutif garantit une adaptation future aux besoins grandissants, assurant ainsi la pérennité et la compétitivité des processus analytiques internes.

\section{Réflexions Finales}

Ce projet illustre l’importance croissante de l’intelligence artificielle et de l’automatisation dans la transformation digitale des entreprises. Il ouvre la voie à de nombreuses pistes d’amélioration, notamment dans l’intégration continue des retours utilisateurs et l’expansion vers d’autres outils BI.

L’expérience acquise constitue un socle solide pour des projets futurs dans le domaine de la data science et du développement d’applications intelligentes (et DevOps). Elle invite également à poursuivre l’exploration des synergies entre IA, automatisation et visualisation pour maximiser la valeur et réduire les charges (temps, matériels, etc) du traitement des données.



\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliographie}

\bibitem{vaswani2017attention} Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). \textit{Attention Is All You Need}. In Advances in Neural Information Processing Systems (NeurIPS), 5998–6008.

\bibitem{touvron2023llama} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., and Grave, E. (2023). \textit{LLaMA: Open and Efficient Foundation Language Models}. arXiv preprint arXiv:2302.13971.

\bibitem{zhang2023llama3} Meta AI (2024). \textit{Introducing LLaMA 3: Open Foundation Models with Enhanced Capabilities}. Meta AI Blog. Available at: \url{https://ai.meta.com/llama} (consulté le 31 mai 2025).

\bibitem{sun2021nl2sql} Sun, W., Wu, Y., Wang, Y., and Wang, S. (2021). \textit{A Survey of Natural Language to SQL Generation}. ACM Computing Surveys (CSUR), 54(3), 1–36.

\bibitem{zhong2017seq2sql} Zhong, V., Xiong, C., and Socher, R. (2017). \textit{Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}. arXiv preprint arXiv:1709.00103.

\bibitem{groq2024} Groq Inc. (2024). \textit{Groq API Documentation and Cloud AI Acceleration}. Disponible sur : \url{https://www.groq.com} (consulté le 31 mai 2025).

\bibitem{groq2023} Groq Inc. (2023). \textit{Groq: AI Inference at the Speed of Thought}. Disponible sur : \url{https://www.groq.com}.

\bibitem{microsoft2023powerbi} Microsoft Corporation (2023). \textit{Power BI Embedded Analytics}. Documentation officielle Microsoft. Disponible sur : \url{https://docs.microsoft.com/en-us/power-bi/developer/embedded/embedding} (consulté le 31 mai 2025).

\bibitem{kimball2013data} Kimball, R., Ross, M., Thornthwaite, W., Mundy, J., and Becker, B. (2013). \textit{The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling}. 3rd Edition, Wiley.

\bibitem{xu2016excel} Xu, J., and Mei, H. (2016). \textit{Automating Excel Tasks with Python: Tools and Techniques}. Journal of Software Engineering and Applications, 9(7), 333–345.

\bibitem{xu2018automating} Xu, W., and Lee, S. (2018). \textit{Automating Excel Data Integration with Mapping Rules}. Proceedings of the International Conference on Data Engineering (ICDE), 1552–1555.

\bibitem{miller2018nlambiguity} Miller, T. (2018). \textit{Explanation in Artificial Intelligence: Insights from the Social Sciences}. Artificial Intelligence, 267, 1–38.

\bibitem{dwork2014algorithmic} Dwork, C., and Roth, A. (2014). \textit{The Algorithmic Foundations of Differential Privacy}. Foundations and Trends® in Theoretical Computer Science, 9(3–4), 211–407.

\bibitem{goodfellow2016deep} Goodfellow, I., Bengio, Y., and Courville, A. (2016). \textit{Deep Learning}. MIT Press.

\bibitem{holtzman2019curious} Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. (2019). \textit{The Curious Case of Neural Text Degeneration}. arXiv preprint arXiv:1904.09751.

\bibitem{wang2020minilm} Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. (2020). \textit{MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}. arXiv preprint arXiv:2002.10957.

\bibitem{johnson2019faiss} Johnson, J., Douze, M., and Jégou, H. (2019). \textit{Billion-scale similarity search with GPUs}. IEEE Transactions on Big Data, 7(3), 535–547.

\bibitem{lewis2020rag} Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Yih, W.-T., Rocktäschel, T., Riedel, S., and Kiela, D. (2020). \textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}. arXiv preprint arXiv:2005.11401.

\end{thebibliography}
% Use \bibliographystyle and \bibliography if using BibTeX


\chapter*{Annexe}
\addcontentsline{toc}{chapter}{Annexe}

\section*{L'agent IA}
Cette annexe présente les extraits de code les plus significatifs de l'agent IA, organisés en sections selon les aspects clés de son fonctionnement : l'ingénierie des prompts, les tâches utilisant l'API Groq, l'orchestration du flux de travail, et la génération augmentée par récupération (RAG). Les extraits sont tirés des modules \texttt{data\_reporting\_agent.py} et \texttt{chatbot.py}.

\subsection*{Exemples du Prompt Engineering}

Cette section montre comment les prompts sont conçus pour générer des requêtes SQL dynamiques à partir de questions en langage naturel, en intégrant des informations sur le schéma de la base de données et des valeurs uniques.

\begin{lstlisting}[style=pythonstyle]
# data_reporting_agent.py: Construction du prompt pour la generation SQL
def _build_sql_prompt(self, question, schema_info):
    available_tables = [row[0] for row in schema_info]
    columns_list = [f'"{table}": {", ".join([r[1] for r in schema_info if r[0] == table])}' for table in available_tables]
    unique_values_str = self.db_utils.fetch_unique_values()
    prompt = (
        f"As a PostgreSQL expert, generate one SQL SELECT query based on: "
        f"Question: '{question}'. "
        f"Database schema: {schema_info}. "
        f"Tables and columns: {columns_list}. "
        f"Unique values for filtering: {unique_values_str}"
        "Requirements: "
        "- Use quoted identifiers for table names (e.g., \"Fs_Forecast_2\"). "
        "- Return ONLY the SQL query wrapped in ```\n...\n```. "
        "- Use ILIKE for case-insensitive filtering. "
        "- Ensure valid PostgreSQL syntax."
    )
    return prompt
\end{lstlisting}

\subsection*{Tâches utilisant les appels à l'API Groq}

Les appels à l'API Groq sont utilisés pour interagir avec le modèle LLaMA 3, notamment pour l'analyse d'intention et la génération de texte. Le code suivant illustre la gestion des appels avec gestion des limites de taux et mise en cache.

\begin{lstlisting}[style=pythonstyle]
# chatbot.py: Appel a l'API Groq avec gestion des limites de taux
def _call_groq(self, prompt, retries=3, backoff=1):
    cache_key = f"{prompt}:{self.model}"
    if cache_key in self.response_cache:
        print(f"{datetime.now()} - Cache hit for prompt: {prompt[:50]}...")
        return self.response_cache[cache_key]
    
    data = {"messages": [{"role": "user", "content": prompt}], "model": self.model}
    for attempt in range(retries * len(self.api_keys)):
        self.current_key_idx = (self.current_key_idx + 1) % len(self.api_keys)
        self.headers["Authorization"] = f"Bearer {self.api_keys[self.current_key_idx]}"
        if not self._check_rate_limit():
            time.sleep(backoff)
            backoff *= 2
            continue
        response = requests.post(self.url, headers=self.headers, json=data)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"]
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            result = json_match.group(0)
        self.response_cache[cache_key] = result
        self.request_counts[self.current_key_idx]["count"] += 1
        return result
\end{lstlisting}

\subsection*{Orchestration du flux de travail}

L'orchestration du flux de travail dans \texttt{DataReportingAgent} gère l'exécution dynamique des tâches, telles que l'extraction de données, la génération de rapports, et l'envoi d'emails, en fonction de l'intention détectée.

\begin{lstlisting}[style=pythonstyle]
# data_reporting_agent.py: Orchestration du flux de travail
def execute_dynamic_workflow(self, question):
    print(f"{datetime.now()} - Starting workflow for: {question}")
    results = {}
    intent = self._analyze_intent(question)
    if intent["data"]:
        with self.chatbot.db_engine.connect() as conn:
            query = text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'")
            available_tables = [row[0] for row in conn.execute(query).fetchall()]
            schema_query = text("SELECT table_name, column_name, data_type FROM information_schema.columns WHERE table_schema = 'public'")
            schema = conn.execute(schema_query).fetchall()
        sql_query = self.chatbot._call_groq(self._build_sql_prompt(question, schema))
        result = pd.read_sql(text(sql_query), self.chatbot.db_engine)
        results['data'] = result if not result.empty else "No records found"
    if intent["report"]:
        report_path = self.report_gen.create_pdf_report(
            f"Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            {"Project Summary": results.get('data')},
            text_sections={"Executive Summary": self.chatbot.generate_report_text(question, results.get('data'))}
        )
        results["report"] = report_path
    return results
\end{lstlisting}

\subsection*{Génération augmentée par récupération (RAG)}

L'implémentation de RAG permet de récupérer des informations contextuelles pour enrichir les réponses, notamment pour la génération de rapports stratégiques, en utilisant une base de connaissances dynamique.

\begin{lstlisting}[style=pythonstyle]
# chatbot.py: Implementation de RAG
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class Chatbot:
    def __init__(self, config_path):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = faiss.IndexFlatL2(384)
        self.doc_store = []

    def add_to_knowledge_base(self, text, metadata=None):
        embedding = self.embedding_model.encode([text])[0]
        self.index.add(np.array([embedding]))
        self.doc_store.append((text, metadata or {}))

    def retrieve(self, query, k=3):
        query_embedding = self.embedding_model.encode([query])[0]
        distances, indices = self.index.search(np.array([query_embedding]), k)
        return [self.doc_store[i] for i in indices[0] if i < len(self.doc_store)]

    def generate_report_text(self, question, df):
        retrieved_docs = self.retrieve(question, k=3)
        context = "\n\n".join([doc[0] for doc in retrieved_docs])
        prompt = (
            f"Given question: '{question}', table columns: {df.columns.tolist()}, "
            f"and context:\n{context}\n"
            "Generate a concise strategic report summary (250-500 words)."
        )
        return self._call_groq(prompt)
\end{lstlisting}

\newpage
\section*{Pipeline de transformation/mappage de données}

Cette annexe documente la pipeline de transformation de données implémentée dans les modules \texttt{file\_loader.py}, \texttt{column\_mapper.py}, \texttt{value\_mapper.py}, et \texttt{data\_transformer.py}. Cette pipeline est conçue pour aligner et transformer des fichiers Excel sources vers un format cible compatible avec la base de données \texttt{staffing}, utilisée dans le projet d'agent IA sécurisé pour NL2SQL basé sur le modèle fine-tuné LLaMA 3 8B.

\subsection*{Chargement des fichiers Excel}

Le module \texttt{file\_loader.py} encapsule la gestion des fichiers Excel via la classe \texttt{ExcelFile}, qui permet de charger, prévisualiser, modifier et sauvegarder des DataFrames pandas.

\begin{lstlisting}[style=pythonstyle]
# file_loader.py: Chargement et manipulation des fichiers Excel
import pandas as pd

class ExcelFile:
    def __init__(self, path: str):
        self.path = path
        self.df = pd.read_excel(path)
        self.name = path.split("/")[-1]

    def get_columns(self):
        return list(self.df.columns)

    def preview(self, n=5):
        return self.df.head(n)

    def filter_columns(self, columns: list):
        self.df = self.df[columns]

    def add_column(self, name: str, default_value=None):
        self.df[name] = default_value
\end{lstlisting}

\subsection*{Mappage des colonnes}

Le module \texttt{column\_mapper.py} définit la classe \texttt{ColumnMapper} pour établir des correspondances entre les colonnes sources et cibles, permettant une conversion flexible des noms de colonnes.

\begin{lstlisting}[style=pythonstyle]
# column_mapper.py: Mappage des colonnes sources vers cibles
class ColumnMapper:
    def __init__(self, source_columns, target_columns):
        self.source_columns = source_columns
        self.target_columns = target_columns
        self.mapping = {}

    def set_mapping(self, source_col, target_col):
        if source_col in self.source_columns and target_col in self.target_columns:
            self.mapping[source_col] = target_col

    def get_mapped_column(self, source_col):
        return self.mapping.get(source_col, None)
\end{lstlisting}

\subsection*{Transformation des valeurs}

Le module \texttt{value\_mapper.py} implémente la classe \texttt{ValueMapper}, qui gère trois types de transformations : mappages directs de valeurs, fractionnement de colonnes, et fonctions personnalisées basées sur des lignes.

\begin{lstlisting}[style=pythonstyle]
# value_mapper.py: Transformation des valeurs
class ValueMapper:
    def __init__(self):
        self.mappings = {}
        self.split_mappings = {}
        self.custom_column_functions = {}

    def set_value_mapping(self, column, from_val, to_val):
        if column not in self.mappings:
            self.mappings[column] = {}
        self.mappings[column][from_val] = to_val

    def set_split_mapping(self, column, new_columns, delimiter):
        self.split_mappings[column] = (new_columns, delimiter)

    def set_custom_function(self, target_column, func):
        self.custom_column_functions[target_column] = func

    def apply(self, df):
        df_transformed = self.apply_mappings(df)
        df_final = self.apply_custom_functions(df_transformed)
        return df_final
\end{lstlisting}

\subsection*{Orchestration de la transformation}

Le module \texttt{data\_transformer.py} orchestre la pipeline via la classe \texttt{DataTransformer}, qui combine les mappages de colonnes et de valeurs pour fusionner les données sources dans le DataFrame cible, en utilisant une clé commune (\texttt{match\_key}).

\begin{lstlisting}[style=pythonstyle]
# data_transformer.py: Orchestration de la transformation
class DataTransformer:
    def __init__(self, column_mapper, value_mapper, match_key):
        self.column_mapper = column_mapper
        self.value_mapper = value_mapper
        self.match_key = match_key

    def transform(self, source_df, target_df):
        mapped_source = pd.DataFrame()
        for src_col, tgt_col in self.column_mapper.mapping.items():
            if src_col in source_df.columns:
                mapped_source[tgt_col] = source_df[src_col]
        mapped_source[self.match_key] = source_df[self.match_key]
        target_df.set_index(self.match_key, inplace=True)
        mapped_source.set_index(self.match_key, inplace=True)
        for idx in mapped_source.index:
            if idx in target_df.index:
                for col in mapped_source.columns:
                    if col in target_df.columns and pd.isna(target_df.at[idx, col]):
                        target_df.at[idx, col] = mapped_source.at[idx, col]
            else:
                target_df.loc[idx] = mapped_source.loc[idx]
        result_df = target_df.reset_index()
        return self.value_mapper.apply(result_df)
\end{lstlisting}

\subsection*{Conclusion}

Cette pipeline offre une solution robuste pour transformer des données Excel dans le cadre du projet d'automatisation du flux entre les fichiers Excel. Son design modulaire permet une adaptation facile à d'autres contextes de transformation de données.


\newpage
\section*{Étude sur le fine-tuning de LLaMA 3 8B}

Cette annexe décrit le processus de fine-tuning du modèle LLaMA 3 8B afin de créer un agent IA local et personnalisé pour la conversion de langage naturel en requêtes SQL (NL2SQL). L'objectif principal est de remplacer l'usage de l'API Groq (basée sur LLaMA 3 70B) par une solution plus légère, hébergeable localement. Bien que le fine-tuning ait abouti à un prototype fonctionnel, les limitations imposées par l’environnement (Google Colab Free Tier) ont restreint la qualité du résultat final.

\subsection*{Motivation et objectifs}

Le modèle vise à interpréter des instructions en langage naturel et générer automatiquement des requêtes SQL valides, en tenant compte de la structure de la base de données \texttt{staffing}. Les principaux objectifs étaient :

\begin{itemize}
    \item \textbf{Sécurité} : assurer un traitement local des données sensibles.
    \item \textbf{Personnalisation} : adapter le modèle à des schémas de tables spécifiques, comme \texttt{fs\_resources\_2} et \texttt{fs\_forecast\_2}.
    \item \textbf{Autonomie} : réduire la dépendance aux services cloud externes.
\end{itemize}

\subsection*{Processus de fine-tuning}

L'entraînement a été effectué sur Google Colab (Free Tier) avec GPU T4 (15 Go VRAM), à l'aide du modèle \texttt{unsloth/Meta-Llama-3.1-8B}, chargé en 4-bit quantization pour rester dans les limites mémoire.

\subsubsection*{Configuration du modèle}

La technique LoRA (Low-Rank Adaptation) a été utilisée pour n'entraîner qu'une fraction des paramètres du modèle. La configuration :

\begin{itemize}
    \item \texttt{r=16} : rang des matrices d’adaptation. Un compromis courant pour un ajustement expressif mais peu coûteux.
    \item \texttt{lora\_alpha=16} : facteur d’amplification des mises à jour LoRA.
    \item \texttt{lora\_dropout=0} : aucun dropout, adapté au petit nombre d'étapes.
    \item Modules ciblés : \texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}, \texttt{gate\_proj}, \texttt{up\_proj}, \texttt{down\_proj}.
\end{itemize}

\begin{lstlisting}[language=Python, style=pythonstyle]
from unsloth import FastLanguageModel
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Meta-Llama-3.1-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=True
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407
)
\end{lstlisting}

\subsubsection*{Préparation des données}

Le jeu de données utilisé est \texttt{gretelai/synthetic\_text\_to\_sql}. Une séparation 90/10 a été effectuée pour l’entraînement et la validation. Les exemples ont été formatés dans un style de prompt type Alpaca pour du supervised fine-tuning.

\begin{lstlisting}[language=Python, style=pythonstyle]
from datasets import load_dataset

dataset = load_dataset("gretelai/synthetic_text_to_sql")
train_valid_split = dataset["train"].train_test_split(test_size=0.1, seed=3407)
train_dataset = train_valid_split["train"]
valid_dataset = train_valid_split["test"]

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
Company database: {}
### Input:
SQL Prompt: {}
### Response:
SQL: {}
"""

def formatting_prompts_func(examples):
    texts = [alpaca_prompt.format(db, prompt, sql) + tokenizer.eos_token 
             for db, prompt, sql in zip(examples["sql_context"], examples["sql_prompt"], examples["sql"])]
    return {"text": texts}

train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
valid_dataset = valid_dataset.map(formatting_prompts_func, batched=True)
\end{lstlisting}

\subsubsection*{Entraînement}

L'entraînement a été réalisé sur seulement 60 étapes avec un batch size effectif de 8 (accumulation de 4 gradients, batch size = 2). L'optimiseur \texttt{adamw\_8bit} a été utilisé pour une gestion mémoire plus efficace. Malgré ce faible nombre d’itérations, le modèle a montré une bonne tendance de convergence.

\begin{quote}
\textbf{Remarque} : Une version plus performante du modèle nécessiterait un entraînement complet (plusieurs époques) sur une infrastructure dotée d’un GPU plus puissant (A100, V100, etc.).
\end{quote}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{wandb.png}
\caption{Courbes de perte sur l’entraînement et la validation (source : Wandb)}
\label{fig:wandb_chart}
\end{figure}

\subsubsection*{Évaluation}

L’évaluation a été réalisée sur 100 exemples. La perplexité observée est raisonnable au vu du peu d'étapes effectuées. Une tentative de scoring automatique via \texttt{evaluate} n’a pas abouti à cause d'une configuration incorrecte du chemin modèle.

\subsubsection*{Exportation}

Le modèle a été exporté au format GGUF (precision \texttt{f16}), pour permettre l’inférence locale via le serveur Ollama :

\begin{lstlisting}[language=Python, style=pythonstyle]
model.save_pretrained_gguf("model", tokenizer, quantization_method="f16")
\end{lstlisting}

\subsection*{Architecture finale de l’application}

L’agent NL2SQL repose sur une architecture modulaire déployable localement :

\begin{itemize}
    \item \textbf{Modèle IA} : LLaMA 3 8B fine-tuné, exécuté via Ollama (\texttt{localhost:11434}).
    \item \textbf{Fine-tunning} : processus générale du fine-tunning.
    \item \textbf{Prompt engineering} : formuler les instructions pour former la pérsonnalité de l'agent.
    \item \textbf{App} : l'application encapsulante de l'agent IA.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fine_tunning.png}
\caption{Architecture Globale basée sur LLaMA 3 8B fine-tuné}
\label{fig:finetune_architecture}
\end{figure}

\subsection*{Conclusion et perspectives}

Cette expérience démontre la faisabilité d’un agent NL2SQL local, basé sur un LLM open source adapté à un cas d’usage métier. Néanmoins, les limitations techniques (environnement gratuit, nombre d'étapes réduit) restreignent ses capacités. Un entraînement plus long, réalisé sur des ressources GPU professionnelles, permettrait d’obtenir un modèle beaucoup plus robuste et performant en production.


\end{document}
